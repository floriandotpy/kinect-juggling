\documentclass[12pt,a4paper,ngerman]{scrartcl}
\usepackage[left=2.5cm,right=2.5cm,top=1cm,bottom=2cm,includeheadfoot]{geometry}
\usepackage{babel}
\usepackage[T1]{fontenc}
\usepackage{verbatim} %muss vorhanden sein für \begin{comment}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{tabularx}

\usepackage{enumitem}

\usepackage[super,square,comma]{natbib}

\usepackage{hyperref}
\hypersetup{
	%linktocpage,
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black}

\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
  {-3.25ex\@plus -1ex \@minus -.2ex}%
  {1ex \@plus .2ex}%
  {\normalfont\normalsize\bfseries}}
\makeatother

\usepackage[usenames,dvipsnames]{color}
\usepackage{listings}
\lstset{
    backgroundcolor=\color[rgb]{0.95,0.95,0.95},
    columns=flexible,
    numbers=left,
    numbersep=5pt,
    numberstyle=\scriptsize,
    basicstyle=\ttfamily\footnotesize,
    captionpos=b,
    keywordstyle=\color{Mahogany},
    commentstyle=\color[rgb]{0.133,0.545,0.133},
    stringstyle=\color{MidnightBlue},
    breaklines=true,
    frame=shadowbox,
    rulesepcolor=\color[rgb]{0.7,0.7,0.7} % color of box-shadow
}

\usepackage{xcolor}

\usepackage{float}
\usepackage{parskip}
%\usepackage[none]{hyphenat}



\begin{document}

\begin{titlepage}

\vspace*{3 cm}

\begin{flushright}

\bfseries{\Huge\scshape Jonglieren mit der Kinect\\{\Large ein Softwareprojekt im}\\\vspace{0.3cm}Projekt Bildverarbeitung}
\end{flushright}

\vspace{2 cm}

\begin{flushright}
\scshape{\large Projektbericht}
\end{flushright}

\begin{flushright}
\scshape{\LARGE\bfseries Rolf Boomgaarden\\}
\scshape{\LARGE\bfseries Thiemo Gries\\}
\scshape{\LARGE\bfseries Florian Letsch\\}
\end{flushright}

\vspace{2 cm}

\begin{flushright}
\scshape{\large\bfseries \today}
\end{flushright}

\vfill

\begin{flushright}

\scshape{Unter Aufsicht von: {\bfseries Benjamin Seppke}\\}
\scshape{\bfseries Arbeitsbereich Kognitive Systeme\\}
\scshape{\bfseries Fachbereich Informatik, Universität Hamburg\\}
\end{flushright}

\end{titlepage}

\tableofcontents
\newpage

\section{Einleitung}

Seit Markteinführung der Microsoft Kinect steht in vielen
Haushalten ein ausgereiftes bildverarbeitendes System im Wohnzimmer. Seit der
Verfügbarkeit des Unterhaltungsgerätes steht auch für an
wissenschaftlich Interessierte ein finanziell erschwingliches Gerät zur Verfügung,
das für vielfältige Aufgaben im Bereich der Bildverarbeitung verwendet werden kann.

Mit den technischen Möglichkeiten dieses Tiefen- und Bilddaten liefernden Systems
soll in dieser Arbeit versucht werden, das Wurfmuster eines mit Bällen jonglierenden
Akteurs zu erfassen und zu analysieren. % analysieren??

Im Rahmen des Masterprojekts Bildverarbeitung haben wir uns zwei Semester lang mit
der Kinect beschäftigt und in einem von Erfolgen und Fehlschlägen gefüllten Prozess
versucht, die fliegenden Bälle eines jonglierenden Akteurs in Echtzeit zu verfolgen.

Ein funktionierendes Endergebnis wurde erzielt, das wir in dieser Arbeit vorstellen.
Zugleich dokumentieren wir unsere Herangehensweise und umreißen relevante Arbeiten
auf dem Gebiet, die uns Anregung gegeben haben. Ausgehend von einer grundsätzlichen
Lösungsidee betrachten wir unseren ersten Ansatz, beschreiben die dabei aufgetretenen
Probleme und führen so nach und nach auf das Endergebnis. Hierbei werden wir auch
über die Implementierung sprechen, die von uns gewählte Softwarearchitektur
erläutern und dokumentieren.

Das Endergebnis wird dann bewertet und aus den zu Anfang genannten Zielen
hinsichtlich des Erfolges evaluiert. % WTF schreibe ich hier?

\newpage

\subsection{Vorüberlegung}

Bevor eine konkrete Zielsetzung getroffen wird, treffen wir ein paar Vorüberlegungen,
die uns zu Projektbeginn begegneten. In der Ideenfindungsphase des Projektes kam uns
sehr schnell die Idee, \textit{irgendwie} die Kinect mit \text{Jonglieren} zu
verbinden.

Dieses \textit{``irgendwie''} war schwer zu greifen und zu definieren, da keiner von
uns Erfahrung im Bereich der Tiefendatenverarbeitung hatte und mögliche
Schwierigkeiten schwer vorauszusehen und abzuschätzen waren.

Die Idee zu einer interaktiven Anwendung zum Trainieren des Jonglierens kam uns
relativ schnell, doch sobald wir anfingen, eine tatsächliche Anwendung konkret zu
umreißen, wurde uns die Komplexität eines solchen Unterfangens bewusst. Um aber
an der uns zusagenden Grundidee weiter zu arbeiten, wollten wir uns im Rahmen des
Projektes mit den bildverarbeitenden Grundkomponenten beschäftigen, die aus
den Bild- und Tiefendaten der Kinect heraus die Jonglierbälle erkennt und verfolgt,
um dann in irgendeiner Form sinnvolle Informationen über die bewegten Objekte aus diesen
Daten zu generieren.

Der Einfachheit halber setzen wir uns zu Beginn mit dem Grundmuster des Jonglierens,
der Kaskade\cite{libraryjuggling} auseinander. Diese entspricht anschaulich einer liegenden Acht. Auch
haben wir uns auf die Anzahl von exakt drei Jonglierbällen festgelegt. Durch diese
Grundannahme haben wir uns eine Vereinfachung beim Erkennen und Verfolgen der Bälle
erhofft. Im Laufe der Arbeit hat sich dann herausgestellt, dass diese starre Annahme
uns zu einer nur wenig robusten Lösung führte. In der am Ende des Projektes
stehenden Lösung ist dann die Anzahl der jonglierten Objekte tatsächlich unerheblich
und das System bestimmt an Hand der berechneten Daten sogar die Gesamtzahl der
im Jongliermuster befindlichen Objekte.

Zunächt aber betrachten wir unser ursprüngliches Modell. In der ersten
Grundüberlegung wird deutlich, dass ein Jongleur Jonglierbälle
in einem Muster wirft, das möglichst gleichmäßig ist.
So ist der Höhepunkt der Flugbahn idealerweise konstant auf einer Höhe.
Bei einer Analyse des Jongliermusters wäre dies also bereits ein erstes Kriterium,
die \textit{Güte eines Jongliermusters} automatisiert zu bewerten.

% FIXME: Das klingt ja schon so nach Jongliertrainer. Das ist ja hier Quatsch
Denkbar sind auch weitere Anwendungen, wie etwa das automatische Zählen von
erfolgreich gefangenen Würfen. Ein einfaches Kriterium für eine \textit{Leistungsbewertung des
jonglierenden Benutzers} ist eine computergesteuerte Erfassung der Wurfanzahl insgesamt.

Die spätere Analyse und Bewertung des Jongleurs ist jedoch nicht Ziel dieser Arbeit. Stattdessen verfahren wir
in einem bottom-up Herangehen, um ausgehend von den rohen Bild- und Tiefendaten der Kinect Informationen über sich im Bild befindliche Objekte (Jonglierbälle) zu erfassen und deren Bewegung zu erkennen und in eine mathematische Beschreibung zu überführen.
Das Ergebnis ist dann ein Fundament, auf dessen Grundlage konkrete Anwendungen entwickelt werden können.

\newpage

\subsection{Projektziel}

Am Ende dieser Arbeit soll eine Anwendung stehen, die mit Hilfe der Kinect Daten
über die Flugbahnen mehrerer jonglierter Bälle liefert.

Ein Akteur befindet sich hierbei im Bildzentrum in einem wohl definierten Abstand
zur Kinect. Um das Ergebnis unabhängig von der Szenenbeleuchtung zu halten, sollen die Tiefendaten ausreichend
Information für das eindeutige Identifizieren der Bälle liefern. Da reflektierende Bälle vom Tiefensensor der Kinect nur schwer erfasst werden können, werden drei matte Bälle beliebiger Farbe jongliert.

Die Anwendung soll eine Grundlage liefern, um auf Grundlage den gewonnen Information
heraus konkrete weitere Anwendungen zu schreiben.


\section{Projektplanung}

\subsection{Möglichkeiten der Kinect}
\label{sec:kinect}

Die Kinect ist eine von Microsoft\cite{microsoft} zur Spielekonsole Xbox 360 vertriebene Erweiterung,
die den Spieler mit einem RGB- und einem Tiefensensor erfasst und diese beiden
Datenströme an die Konsole liefert. Da die Kinect über einen USB-Anschluss verfügt,
kann sie an konventionellen Rechnern angeschlossen und betrieben werden. Eine
quelloffene Implementierung zur Unterstützung der Kinect ist das Open Kinect Projekt,
das für Linux, Windows und MacOS zur Verfügung steht und im Rahmen dieser Arbeit
als Bibliothek für Python verwendet wird.\cite{openkinect}\cite{libfreenect}

Die Videoquelle der Kinect liefert standardmäßig 30 Bilder pro Sekunde mit einer
Auflösung von 640x480 und 8 bit Farbtiefe. Tatsächlich kann sie mit
einer Auflösung von 640x512 aufnehmen (oder sogar 1280x1024 und 10 fps), aber die
Auflösung wird an die der Tiefenkamera angepasst, so dass die reduzierte Auflösung
ausgegeben wird. Die Tiefendaten stammen von einer
Infrarot-Kamera und liefern bei gleicher Bildfrequenz 2048 verschiedene
Tiefenwerte (11bit). Das Tiefenbild deckt einen Bereich von 0,8 m bis 3,5 m von der
Kinect aus ab, bei der in 2m Entfernung eine Auflösung von 3 mm in der Bildebene und
1 cm in der Tiefenebene erreicht wird. Bedingt durch die Funktionsweise der Infrarotkamera
ergibt sich, dass die Kinect in Umgebungen starker Infrarotstrahlung (beispielsweise
im Tageslicht) nur beschränkt einsatzfähig ist.\cite{hacking}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.33]{img/kinect_components.jpg}
    \caption{Aufbau der Kinect 1.0. ({\em Bildquelle: Hacking the Kinect\cite{hacking}})}
\end{figure}

Um ein Tiefenbild zu erhalten, besitzt die Kinect zwei Hardware-Komponenten, einen
Infrarot-Laser und eine Infrarot-Kamera. Der IR-Laser strahlt Licht mit einer Wellenlänge
von 830 nm aus, welches ein der Kinect bekanntes Rausch-Muster erzeugt. Dieses Rausch-Muster enthält
mitunter neun hellere Punkte, die über das Muster verteilt sind.\\
Die Kinect nutzt die Streifenprojektion (auch Streifenlichttopometrie genannt), um ein
räumliches Bild zu erfassen. So wird das vom Laser erzeugte bekannte Rauschmuster mit der IR-Kamera erfasst, die in einem festen Abstand zum Laser befestigt ist. Weicht das aufgenommene
Bild von dem bekannten Rausch-Muster ab, wird davon ausgegangen, dass dies durch
Objekte im Raum verursacht wird, die das Muster stören. Mit Hilfe der neun helleren Punkten werden
auch alle anderen Punkte zugeordnet und es kann ein räumliches Bild errechnet werden.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.33]{img/kinect_infrared.jpg}
    \caption{Infrarot-Punktmuster der Kinect 1.0. ({\em Bildquelle: Our First Look at the Xbox One Kinect IR Field\cite{kinect2:infrared}})}
\end{figure}

Mögliche Störungen entstehen durch Varianz in der Stärke der Laserdiode und der Wellenlänge des Lichts.
Die IR-Kamera der Kinect hat einen Infrarot-Pass-Filter, der nur Licht im Bereich von 830 nm
hindurch lässt, um nicht durch anderes Licht (wie beispielsweise Fernbedienungen) geblendet zu
werden. Dennoch gibt es Lichtquellen, wie das Sonnenlicht, welches genügend Licht mit einer
Wellenlänge von 830 nm  beinhaltet, um die Kinect überblenden zu können. So kann bei starkem
Tageslicht oder im Freien mit Fehlverhalten bis hin zur Unnutzbarkeit der Kinect gerechnet werden.

\subsubsection{Beispielprojekte mit der Kinect}

Mit der Kinect lassen sich viele Ideen umsetzen. So gibt es ein Projekt, bei dem Spielzeugautos und
-figuren vor der Kinect verschoben werden, eine Software erkennt diese und stellt diese Bewegungsabläufe
in einer virtuellen Welt als Animation dar.\cite{3dpuppetry} Ein weiteres Projekt beschäftigt sich
damit, ein 3D-Modell eines Menschen mit nur einer Kinect aufzunehmen.\cite{kinectavatar} Ein anderes Projekt hat eine an einer Kamera
befestigten Kinect mit zusätzlichem Bewegungssensor, welche die Bewegungen beim Fotografieren messen, um die
entstehende Bewegungsschärfe aus dem Foto hinauszurechnen.\cite{motiondeblurring} Eine Forschungsgruppe
hat einem humanoiden Roboter mit der Kinect die Fähigkeit verliehen, ihm zugeworfene Bälle zu fangen.\cite{kober}
Ein weiteres Projekt in der Videographie nimmt mit der Kinect RGBZ-Videos auf, in der nachträglich beispielsweise die
Szenenbeleuchtung neu berechnet werden kann.\cite{rgbzvideos}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{img/kinectavatar.jpg}
    \caption{Beispielprojekt: Automatischer Ganzkörperscan mit nur einer Kinect. ({\em Bildquelle: KinectAvatar: Fully Automatic Body Capture Using a Single Kinect\cite{kinectavatar}})}
\end{figure}

\subsubsection{Stand 2014: Kinect for Xbox One}
Während der Projektzeit, im November 2013, ist die Xbox One und damit auch die Kinect for Xbox One, oder auch Kinect 2.0, erschienen.\cite{kinect2:verge} Die neue Kinect beinhaltet bessere Hardware und auch softwaretechnisch wurden viele Features implementiert, so dass das Erkennen von Bewegungen sehr viel feiner funktionieren soll. Für dieses Projekt würde sie somit viele Vorteile mit sich bringen, wir haben sie jedoch nicht genutzt.

\subsection{Recherche: Ein jonglierender Roboter}

Im Vorfeld der ersten eigenen Implementierungsversuche sind wir bei der
Recherche auf eine Arbeit von Jens Kober, Matthew Glisson und Michael Mistry
gestoßen, die zumindest in Teilen eine ähnliche Aufgabenstellung verfolgte.
Unter dem Titel {\em Playing Catch and Juggling with a Humanoid Robot}\cite{kober}
erstellte die Gruppe des Disney Research Center\cite{disneyresearch} einen
humanoiden Roboter, der auf ihn zugeworfene Bälle fängt und zurückwirft.
Teil der Gesamtaufgabe in der Entwicklung dieses Systems war ein
bild- und tiefendatenverarbeitendes System, das mit einer der Kinect sehr ähnlichen
Kamera arbeitete.

Kernidee dieses Systems ist eine \textit{Image Processing Pipeline}, also eine
Kette von Verarbeitungsschritten, die als Eingabe eine Folge von RGB- und Tiefendaten
nimmt und als Ausgabe die aktuelle Position und die berechnete Flugbahn für die Zukunft liefert.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.31]{img/koberpipeline.jpg}
    \caption{Bildverarbeitungs-Pipeline von Kober, Glisson und Mistry. ({\em Bildquelle: Playing Catch and Juggling with a Humanoid Robot\cite{kober}})}
    \label{koberpipeline}
\end{figure}

Die Aufgabe, die in dieser Arbeit gelöst wurde, ist also von der Grundidee her eine
ähnliche, weshalb wir den Grundaufbau der Verarbeitungsschritte auch in unserem
Vorgehen übernehmen wollten. Wie wir im Laufe der Programmierung aber feststellten,
hatte die Arbeit einige Rahmenbedingungen anders gesetzt, so dass wir auf Probleme
stießen, die sich in der Arbeit mit dem Roboter offensichtlich nicht so deutlich
zeigten.

Hierbei ist zuerst zu nennen, dass die betrachteten Bälle bei uns sehr viel kleiner
waren, da wir einen tatsächlich jonglierenden Menschen betrachtet haben. In der
Arbeit mit dem Roboter wurden deutlich größere Bälle verwendet, was deren Erkennung
entsprechend vereinfacht.

Da in diesem Projekt die Bälle von einer Person dem Roboter zugeworfen werden,
ergeben sich auch längere Wurfbahnen und eine längere Flugzeit pro Ball. Wie
später in der Arbeit zu sehen sein wird, ist das normale Jongliermuster mit zwei
Händen teilweise so klein, dass es schwer wird, dicht aneinander vorbei fliegende
Bälle voneinander zu unterscheiden.

Zusätzlich wurde in der Arbeit mit dem Roboter auf verschiedenfarbige Bälle
zugegriffen, um diese voneinander zu unterscheiden. Dies ist eine Rahmenbedingung,
die wir so nicht wählen wollten, da die RGB-Werte, die die Kinect liefert, sehr stark
von den Beleuchtungsbedingungen abhängen und bei den verwischten Objekten, wie
wir sie in den Kinect-Aufnahmen sehen, keine robuste Erkennung möglich ist.


\subsection{Lösungsidee}

Als erste Lösungsidee haben wir in Anlehnung an die betrachtete Arbeit mit dem
jonglierenden Roboter selbst eine Reihe von durchzuführenden Bildverarbeitungsschritten
zusammengesetzt und erarbeitet, wie in Abbildung \ref{ourpipeline} zu sehen ist.

Die Verarbeitung wird auf den Einzelbildern durchgeführt, die Ergebnisse der
einzelnen Bilder werden dann kombiniert.

\begin{enumerate}
\item Auf den Tiefendaten werden ab einem bestimmten Tiefenwert (\textit{threshold})
	alle Tiefeninformationen abgeschnitten. Jongleur und fliegende Jonglierbälle sind
	somit freigestellt.
\item Auf den reduzierten Tiefendaten werden nun lokale Maxima bestimmt, in welchen die
	Jonglierbälle, aber auch die Hände enthalten sind, da sie sich näher an der Kinect befinden als der
	Jongleur.
\item Die gefundenen Maxima sind mögliche Kandidaten für tatsächlich erkannte
	Jonglierbälle.
\item Die entsprechenden Regionen werden als Maske für die RGB Daten verwendet.
\item In den interessanten Regionen der RGB Daten wird eine Erkennung für runde
	Objekte durchgeführt. Dies wird mit einer \textit{Hough Transformation} gelöst.
\item Die nun erkannten Bälle werden aufgeteilt und den einzelnen Bällen zugeordnet.
\item Für jeden Ball liegt nun also eine Reihe von Positionen vor. Um zu einer
	flüssigen Flugbewegung zu gelangen wird die Flugbahn jedes Balls mit Hilfe eines
	\textit{Kalman-Filters} modelliert und laufend aktualisiert. Mit dieser stetigen
	Positionsinformation kann die Position eines Balles also programmseitig jederzeit
	abgefragt werden.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.14]{img/processing-pipeline.png}
    \caption{Unsere Bildverarbeitungs-Pipeline im ersten Entwurf.}
    \label{ourpipeline}
\end{figure}

Wie sich im Laufe des Projektes heraus gestellt hat, ist dieses Konzept in einigen Schritten zu idealisiert und in anderen unnötig
kompliziert. Bevor wir aber in späteren Abschnitten auf diese nicht funktionierenden
Ansätze eingehen, wollen wir zuerst unsere finale Umsetzung vorstellen.

\section{Finale Umsetzung}
\label{sec:final}

Im Folgenden stellen wir die finale Lösung vor, welche das Endergebnis des
Projektes darstellt. Die zahlreichen Ansätze, die wir währenddessen verfolgt haben, um zu
diesem Punkt zu gelangen, stellen wir anschließend in Kapitel \ref{sec:ansaetze} vor.

\subsection{Programmstruktur}

Das Programm besteht im Kern aus einer Schleife, in der die folgenden vier Schritte  wiederholt werden:
\begin{enumerate}
  \item Benutzereingabe verarbeiten
  \item Aktuelle Bild- und Tiefendaten abfragen (Von einer angeschlossenen Kinect oder aufgezeichneten Demodaten)
  \item Bild- und Tiefendaten verarbeiten (mehr dazu im Kapitel Programmfluss, siehe \ref{sec:programmfluss})
  \item Verarbeitete Bilddaten anzeigen
\end{enumerate}

Beim Starten des Programms über die Konsole wird zunächst die Kinect initialisiert.
Im Normalfall ist eine Kinect angeschlossen, welche dann mit Hilfe der freenect Bibliothek \cite{libfreenect} angesteuert wird. Diese liefert dann die Bild- und Tiefendaten der Kinect Kamera bzw. des Tiefensensors. Wenn keine reale Kinect zur Verfügung steht, kann alternativ auch eine Dummy-Kinect-Instanz die RGB- und Tiefenbilder bereitstellen. In diesem Fall werden zuvor aufgenommene Bildsequenzen in einer Schleife zurückgegeben um so den Datenfluss einer echten Kinect zu simulieren.
Im Anschluss daran werden sämtliche Filter initialisiert, die in der Verarbeitungsphase zum Einsatz kommen sollen.

Welche Filter aktiviert und ob eine reale Kinect verwendet oder stattdessen simuliert werden soll, kann beim Programmaufruf über die Konsole festgelegt werden. Dazu wird eine beliebige Anzahl an Parametern an das Programm übergeben. So gehört zu jedem Filter ein entsprechender Kommandozeilen-Parameter, durch den der Filter aktiviert werden kann (für eine vollständige Auflistung der verfügbaren Kommandozeilenparameter  siehe Anhang \ref{sec:parameter}). Dies ermöglicht es, verschiedene Filter je nach Bedarf miteinander zu kombinieren. Gleichzeitig können leicht auch nachträglich noch neue Filter hinzugefügt werden. Zudem gibt es Parameter um die Kinect zu simulieren oder Daten von einer echten Kinect aufzunehmen um diese zu einem späteren Zeitpunkt von der simulierten Kinect wiedegeben lassen zu können. Die Reihenfolge der Parameter hat hierbei keine Bedeutung und wirkt sich nicht auf die Ausführungsreihenfolge der Filter aus.

Der Aufruf für die optimale Filterkombination unseres Endergebnisses erfolgt durch \lstinline{python main.py --detectball --cutoff  --dummymode --handtracking --simplehand}

\begin{tabular}{llp{8cm}}
Parameter & Aktivierter Filter & Erklärung \\ \hline
{\lstinline!--dummymode!} & - & Verwendung von Testdaten statt einer angeschlossenen Kinect \\
{\lstinline!--cutoff!} & {\lstinline!CutOffFilter!} & Tiefenbild an Hand eines festen Schwellwertes binarisieren \\
{\lstinline!--detectball!} & {\lstinline!RectsFilter!} & Vorverarbeitung für die Ballerkennung: Regionen im Tiefenbild segmentieren \\
{\lstinline!--handtracking!} & {\lstinline!HandTrackingFilter!} & Erkennen der Hände vor der Ballerkennung \\
{\lstinline!--simplehand!} & {\lstinline!SimpleHandBallFilter!} & Ballerkennung mit simpler Zuordnung von Bällen zu Ballpositionen
\end{tabular}

Eine vollständige Auflistung der verfügbaren Kommandozeilenparameter folgt im Anhang \ref{sec:parameter}.

Nach der Initialisierungsphase beginnt die oben angesprochene Programmschleife.
In jedem Schleifendurchlauf wird zunächst auf Benutzereingaben reagiert.
Ein simples Abfragen aktueller Tasteneingebaben ermöglicht es dem Benutzer einen Screenshot des momentanen Frames inklusive der eingezeichneten Visualisierung zu speichern (Leertaste), das Programm zu pausieren (p) oder ganz zu beenden (beliebige andere Taste).

Im nächsten Schritt wird nun entweder von einer realen oder simulierten Kinect ein RGB- und ein Tiefenbild abgefragt. Zudem wird eine leere Liste erzeugt, in der später die Ballpositionen gespeichert werden.

Der Kern und gleichzeitig komplexeste Teil der Anwendung ist die Verarbeitung der Bild- und Tiefendaten.
Hierzu werden der Reihe nach sämtliche aktiven Filter auf das RGB- und Tiefenbild sowie die Ballpositionen angewandt.

Dazu verfügt jeder Filter nun über eine \lstinline{filter} Methode mit drei zwingenden und einem optionalen
Parameter. Der optionale Parameter \lstinline{args} ist ein Python Dictionary und wird etwa für das 
Ablegen von Meta-Informationen verwendet, wie etwa Zusatzinfos zur späteren Visualisierung.

Als Ergebnis von jedem Filter wird analog zum Filteraufruf ein Drei-Tupel zurückgegeben, bestehend aus RGB- und Tiefenbild sowie der Liste von Ballpositionen. Die so erhaltenen Daten dienen wiederum dem nächsten Filter als Eingabe-Parameter. Somit können die Filter nach und nach die Rohdaten manipulieren und verarbeiten.

Ein minimaler Filter ohne Funktionalität sieht demnach wie folgt aus.

\begin{lstlisting}[language=Python,caption={Grundstruktur eines Filters. In diesem Beispiel werden die Eingabedaten unverändert zurückgegeben.}]
class DummyFilter(object):

    def filter(self, rgb, depth, balls, args = {}):

        # ...

        return rgb, depth, balls
\end{lstlisting}

Zur besseren Übersicht über die Vielzahl der Filter haben wir diese für die finale Umsetzung in Packages organisiert. Jeder Filter liegt dabei in einem eigenen Modul vor (genauso wie jede der Ballerkennungsmethoden, auf die wir später zu sprechen kommen). Die Strukturierung in fünf Packages je nach Aufgabe der einzelnen Komponenten erlaubt eine übersichtliche Codebasis und selbsterklärende \lstinline{import} Statements.

\begin{tabular}{ll}
\textbf{Package} & \textbf{Inhalt} \vspace{0.1cm} \\
{\lstinline!src.kinect!} & Module zum Ansprechen der Kinect und für Demodaten \\
{\lstinline!src.preprocessing!} & Filter der Datenvorverarbeitung\\
{\lstinline!src.balldetection!} & Hand- und Ballerkennung\\
{\lstinline!src.visual!} & Visualisierung und finale Ausgabe der Daten\\
{\lstinline!src.application!} & Beispielhafte Anwendungen auf Basis unseres Projekts
\end{tabular}

Nachdem alle Filter ausgeführt wurden, werden die resultierenden Bilder angezeigt. Beim Programmstart kann hierbei auch wieder über einen Parameter festgelegt werden, ob entweder das RGB- oder das Tiefenbild angezeigt werden soll. Damit ist ein Schleifendurchlauf beendet und der nächste Durchgang beginnt mit dem Laden der nächsten Bilddaten.

\subsection{Programmfluss}
\label{sec:programmfluss}

Die Verarbeitung der Bild- und Tiefendaten lässt sich in mehrere Schritte unterteilen.

\begin{enumerate}
	\item Tiefendaten vorverarbeiten
	\item Interessante Regionen im Tiefenbild extrahieren
	\item Bälle in Frame-Folgen einander zuordnen
	\item Gewonnene Information visualisieren
\end{enumerate}

Die einzelnen Arbeitsschritte werden im Folgenden noch detaillierter betrachtet.

\subsubsection{Schritt 1: Tiefendaten vorverarbeiten}\label{sec:preprocess}

Die Kinect liefert Tiefenwerte im Bereich von 0 bis 4095 (11 bit), wobei ein höherer Wert
einen größeren Abstand bedeutet. Die 0 steht hierbei für einen Pixel, in dem keine
Tiefeninformation vorliegt.

Für unsere Aufgabe ist Beschaffenheit des Hintergrundes irrelevant. Außerdem nehmen
wir einen isoliert stehenden Jongleur an. Zwischen der Tiefenebene des Jongleurs
und dem Sensor befinden sich laut unsere Annahme keine Objekte außer den
Jonglierbällen. Ein erster Schritt besteht daher im Binarisieren der Tiefendaten.
Ab einem festen Abstand zur Kinect werden alle weiter entfernt liegenden Werte auf 0
(=schwarz) gesetzt. Alle näher am Sensor befindlichen Punkte werden auf 4095 (=weiß)
gesetzt. Der Schwellwert, an dem diese Binarisierung getroffen wird, ist von uns
zuerst auf 2100 gesetzt worden, da dies in den von uns aufgezeichneten Testdaten
genau einer Grenze direkt vor dem Körper des Jongleur entsprach, so dass wir
die Bälle und Hände des Jongleurs isoliert hatten. Im weiteren Verlauf haben wir mit
einer dynamischen Bestimmung dieser Grenze experimentiert, letzten Endes dann aber
doch diesen festen Wert belassen. Bei der tatsächlichen Anwendung in Echtzeit
positioniert man sich also an einen festgelegten Punkt im Raum. Dies ist tatsächlich
auch nicht störend. Da direktes visuelles Feedback von unserer Anwendung deutlich
macht, wann man richtig steht, ist auch keine feste Bodenmarkierung nötig.

{\color{red}FIXME: Bild Tiefendaten -> Binarisiertes Bild}

Auf Grund der Funktionsweise der Kinect mit dem RGB-Sensor und dem knapp daneben
befindlichen Infrarotsensor sind die RGB- und Tiefendaten leicht gegeneinander
verschoben. Hierbei hängt die Verschiebung vom Abstand eines Raumpunktes zur Kinect
ab (Parallaxeffekt). Zwar arbeitet unsere Analyse nur auf den gelieferten
Tiefendaten, so dass wir für die Ballerkennung diese Verschiebung ignorieren
könnten. Für die Visualisierung überlagen wir aber die gewonnene Information mit
den RGB-Daten, so dass wir hierfür diesen Fehler korrigieren müssen. Diese
Korrektur ist daher auch Teil der Vorverarbeitung. Glücklicherweise unterstützt
die \lstinline{freenect} Bibliothek diese Fehlerkorrektur von Haus aus. Nötig
ist hierzu lediglich die Angabe eines bestimmten Formats beim Anfordern eines
Frames von Tiefendaten: \lstinline{freenect.get_depth(format=4)}.
Erzeugt dieser Aufruf einen Fehler, so ist dies ein Hinweis, dass nicht die korrekte
Version der \lstinline{freenect} Bibliothek verwendet wird.

Durch die Funktionsweise der Kinect kann keine Tiefeninformationen an Flächen gewonnen werden, die orthogonal zum Sensor der Kinekt stehen, da dort kein Infrarotlicht zum Sensor reflektiert wird.
Das Tiefenbild besitzt an diesen Stellen dementsprechend keine verwendbaren Tiefenwerte. Stattdessen wird an diesen Positionen der Wert 0 eingefügt. Dies führt an den meisten Objekt-Kanten und insbesondere bei den Jonglierbällen zu einem um das Objekt liegenden Rahmen von Null-Werten. Durch das zuvor angesprochene Binarisieren, werden diese Werte dann im Zuge der Vorverarbeitung auf den maximalen Tiefenwert gesetzt. Aufgrunddessen schrumpfen die Objekte um wenige Pixel. Für die Bälle bedeutet dies, dass deren Durchmesser im Tiefenbild etwas kleiner ist, als der im RGB-Bild. Dies hat aber nur minimale Auswirkungen auf die spätere Erkennung und Bestimmung der Position der Jonglierbälle.
Das Füllen dieser Löcher im Tiefenbild kann alternativ auch kontext-sensitiv ausgeführt werden. Da die momentane Behandlung jedoch zufriedenstellende Ergebnisse liefert, wurde auf einen komplexeren Algorithmus verzichtet. Dieser würde die Ballerkennung wahrscheinlich nur unwesentlich verbessern und würde somit nur unnötig Rechenzeit beanspruchen.

\subsubsection{Schritt 2: Regions Of Interest isolieren}
\label{sec:roi}

Um den Hintergrund zu entfernen und die Bälle freizustellen, wurde folgende Annahme getroffen: Es werden möglichst keine Objekte in der Tiefenebene zwischen dem Spieler und der Kinect platzieren, auch nicht am Rand. Zusätzlich wird davon ausgegangen, dass der Spieler auf einer Linie oder ähnlichem in einem festen Abstand zur Kinect steht.

Die Tiefenwerte werden daher ab einem gewissen Wert einfach abgeschnitten, so dass in diesen im optimalen Fall nur noch die Bälle sowie die Hände des
Jongleurs verbleiben. Das so in seiner Information reduzierte Tiefenbild wird nun noch binarisiert, so dass
es nur noch den einfarbigen Hintergrund und die davor hervorgehobenen Objekte (Hände und Bälle) enthält.



\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{img/depth-binary.png}
    \vspace{-0.5cm}
    \caption{Das vorverarbeitete Tiefenbild. Isoliert zu sehen: Hände und fliegende Bälle}
    \label{rects-3}
\end{figure}


Ein weiterer Ansatz mit einem komplexeren temporalen Filter zum Herausfiltern von unbewegten Objekten und
Hindernissen wird in Abschnitt \ref{sec:temporal} beschrieben. Wir stellten fest, dass der simple Ansatz
völlig ausreichend für unsere Zwecke ist und sogar robuster funktioniert, weshalb wir bei diesem geblieben
sind.

\subsubsection{Schritt 3: Bälle in Frame-Folgen einander zuordnen}
\label{sec:ballzuordnung}

Bevor potentielle Ballpositionen konkreten Ball-Instanzen zugeordnet werden, findet eine kurze Vorverarbeitung statt, bei der Regionen im binarisierten Ergebnis des vorherigen Schrittes als Rechtecke umzeichnet werden, wofür die OpenCV-Funktion \lstinline{cv.BoundingRect} verwendet wird, die genau diese Aufgabe erfüllt. Zu kleine Rechtecke und solche, die den Bildrahmen berühren, werden hierbei bereits herausgefiltert.

Durch die resultierende Liste von Rechtecken wird iteriert und jeder
Rechteckmittelpunkt als mögliche Ballposition aufgefasst (die kürzere Rechteckskante
wird als Balldurchmesser gespeichert).

\vspace{0.5cm}
\begin{lstlisting}[language=Python,caption={RectsFilter.py, Ausschnitt}]
    def filter(self, rgb, depth, balls, args = {}):

        # We'll need open CV for this.
        rgb_cv = cv.fromarray(np.array(rgb[:,:,::-1]))
        depth_cv = cv.fromarray(np.array(depth[:,:], dtype=np.uint8))

        storage = cv.CreateMemStorage(0)
        contour = cv.FindContours(depth_cv, storage, cv.CV_RETR_CCOMP, cv.CV_CHAIN_APPROX_SIMPLE)
        points = []

        ball_list = [] # collect ballpositions in loop
        while contour:
            x,y,w,h = cv.BoundingRect(list(contour))
            contour = contour.h_next()

            # filter out small and border touching rectangles
            t = 2 # tolerance threshold
            minsize = 5
            if x > t and y > t and x+w < self.WIDTH - t and y+h < self.HEIGHT - t and w > minsize and h > minsize:
                x -= 5
                y -= 5
                w += 10
                h += 10
                x, y = self._nullify(x), self._nullify(y) # make sure nothing is smaller than 0

                ball_center = (x+w/2, y+h/2)
                ball_radius = min(w/2, h/2)
                ball_list.append(dict(position=ball_center, radius=ball_radius))


        # and back to numpy...
        rgb = np.copy(rgb_cv)[:,:,::-1]

        return rgb, depth, ball_list
\end{lstlisting}

Es liegt nun für jeden Frame eine Liste von möglichen Ballpositionen vor. Diese setzen sich zusammen aus tatsächlichen Ballpositionen, Positionen, in denen sich die Hände befinden, sowie potentiell weiteren Hindernissen im Sichtfeld der Kinect (wobei wir letztere in unseren Testdaten vermieden haben). Bei den tatsächlichen Ballpositionen ist zu beachten, dass diese etwas ungenau sind und mitunter mehrere Pixeln vom tatsächlichen Ballmittelpunkt abweichen können.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/rects-1.png}
    \vspace{-0.5cm}
    \caption{Hervorhebung erkannter Regionen, in denen sich Bälle und Hände befinden.}
    \label{rects-1}
\end{figure}

\begin{comment}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/rects-2.png}
    \vspace{-0.5cm}
    \caption{FIXME}
    \label{rects-2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/rects-3.png}
    \vspace{-0.5cm}
    \caption{FIXME}
    \label{rects-3}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/rects-4.png}
    \vspace{-0.5cm}
    \caption{FIXME}
    \label{rects-4}
\end{figure}
\end{comment}

Im Folgenden kommt die Hauptaufgabe dieses Schritts: Das Zuordnen der potentiellen
Punkte zu tatsächlichen Ballinstanzen, wobei diese Zuordnung über mehrere Frames
hinweg einem bestehenden Ball eine neue Position zuordnen soll.

Dies ist, wie es sich herausgestellt hat, der aufwändigste Schritt - zumindest der, mit dessen Lösung wir die meiste Zeit verbracht haben. Mehr dazu im später folgenden Abschnitt \ref{sec:ansaetze} zu den von uns ausprobierten Ansätzen.

Unsere endgültige Version der Ballerkennung ist in \lstinline{SimpleHandBallFilter}
implementiert. Robust funktioniert dieser Filter nur, wenn vorher der Filter
\lstinline{HandTrackingFilter} angewandt wurde. \lstinline{HandTrackingFilter}  betrachtet die gefundenen potentiellen Ballpositionen und identifiziert die beiden Hände des Jongleurs. Die als Hände identifizierten Positionen werden aus der Liste entfernt, so dass \lstinline{SimpleHandBallFilter} im Folgenden auf den übrig gebliebenen Positionen arbeitet. Hierdurch kann sehr viel zuverlässiger bestimmt werden, welche Position tatsächlich ein fliegender Ball ist. Bälle, die sich momentan in der Hand des Jongleur befinden, werden hierbei ignoriert bzw. nicht als Bälle erkannt.

Wir wollen diese beiden Filter im Folgenden im Detail betrachten.

Bei Initialisierung des Filters \lstinline{HandTrackingFilter} werden zwei \lstinline{Hand} Instanzen an beliebiger Initialposition erstellt.

\begin{lstlisting}[language=Python,caption={\lstinline{HandTracking.py}, Ausschnitt}]
self.left = Hand((100, 100))
self.right = Hand((200, 200))
\end{lstlisting}

In einem Filteraufruf wird das übergebene \lstinline{args} Dictionary dann
verwendet, um zu markieren, dass der Filter die Handobjekte zugeordnet hat. Es
werden außerdem Referenzen zu den Instanzen für beide Hände gesetzt. Diese Information ist
vor allem für die spätere Visualisierung (optionales Einzeichnen der Handpositionen)
nötig, kann aber auch für andere Weiterverarbeitung verwendet werden.

\begin{lstlisting}[language=Python,caption={\lstinline{HandTracking.py}, Ausschnitt}]
args['hands'] = True
args['hand_left'] = self.left
args['hand_right'] = self.right
\end{lstlisting}

Um die Hände in der Eingabeliste \lstinline{positions} von potentiellen Ball-
Positionen zu identifizieren, wird
die Mitte des Jongliermusters bestimmt. Dies ist der Mittelwert aller x-Koordinaten
der in der Liste befindlichen Positionen.

\begin{lstlisting}[language=Python,caption={\lstinline{HandTracking.py}, Ausschnitt}]
# determine center of juggling pattern
if len(positions) > 0:
    centerX = sum([b['position'][0] for b in positions]) / len(positions)
else:
    centerX = 320
args['centerX'] = centerX
\end{lstlisting}

An Hand dieses Mittelwertes werden die Positionen aufgeteilt in alle Positionen
rechts und alle Positionen links von diesem Wert. In jedem der beiden resultierenden
Liste wird dann die jeweils unterste Position als Hand identifiziert.

\begin{lstlisting}[language=Python,caption={\lstinline{HandTracking.py}, Ausschnitt}]
# two properties for match each hand (left & right)
handProps = [(self.left, lambda p: p['position'][0] <= centerX),
    (self.right, lambda p: p['position'][0] > centerX)]

for hand, filterKey in handProps:
    # filter in right and left positions
    filtered = filter(filterKey, positions)
    if len(filtered) > 0:
        lowest = sorted(filtered, key=lambda p: -1*p['position'][1])[0]
        lowestPosition = lowest['position']

        hand.update(lowestPosition)
        positions.remove(lowest)
\end{lstlisting}

Entscheidend ist das abschließende Entfernen der identifizierten Hand aus der
Liste \lstinline{positions}. Diese Liste wird vom Filter zurück gegeben und im
nachfolgenden Filter \lstinline{SimpleHandBallFilter} verwendet. Für diesen Filter
ist es dann entscheidend, dass sich in der Liste nur noch Positionen von fliegenden
Bällen befinden.


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.45]{img/handtracking-2.png}
    \vspace{-0.5cm}
    \caption{Von \lstinline{HandTrackingFilter} identifizierte Hände, als gelbe Kreise eingezeichnet. Die in jedem Frame bestimmte Mitte des Jongliermusters ist als graue Linie eingezeichnet.}
    \label{rects-1}
\end{figure}

Sind die Hände erkannt und aus der Positionsliste entfernt, ist dann also die
Aufgabe des folgenden Filters \lstinline{SimpleHandBallFilter}, bereits existierende Bälle aus dem vorherigen Frame in ihrer Position zu aktualisieren, sowie Bälle, die am Ende ihrer Flugbahn angekommen sind, aus der Liste der existierenden Bälle zu entfernen, und außerdem neue Flugbahnen zu initialisieren für noch nicht verfolgte Bälle.

Wird \lstinline{SimpleHandBallFilter} zu Programmstart initialisiert, wird zuerst eine leere Liste von Bällen angelegt, da ja noch nichts erkannt wurde.

\begin{lstlisting}
 self.balls = []
\end{lstlisting}

In jedem Filteraufruf wird über die bereits erkannten Bällen aus dem vorherigen Frame iteriert und versucht, sie den potentiellen Ballpositionen aus dem aktuellen Frame zuzuordnen.
Bälle aus dem vergangenen Frame, für die keine passende Folgeposition gefunden wurde, werden aus der Liste der aktuell vorhandenen Bälle entfernt.
Entsprechend wird an einer Position ein
neuer Ball initialisiert, falls die Position keinem bereits erkannten Ball zugeordnet werden konnte.

\begin{lstlisting}[language=Python,caption={\lstinline{SimpleHandBall.py}, Ausschnitt}]
def filter(self, rgb, depth, ball_positions, args={}):

    # try to update balls from last frame, remove non-updated balls
    for ball in self.balls:

        # remember which balls have been updated in this frame
        updated = dict(zip(self.balls, [False for _ in self.balls]))

        # find match from ball <-> position
        for new_ball in ball_positions:
            if ball.isClose(new_ball, future=False):
                ball.updatePosition(new_ball['position'])
                updated[ball] = True
                ball_positions.remove(new_ball)

        # get rid of balls where we couldn't find a matching position
        if not updated[ball]:
            self.balls.remove(ball)

    # create new balls at unused positions
    for ball in ball_positions:
        self.balls.append(SimpleBall(ball['position'], radius=ball['radius']))

    return rgb, depth, list(self.balls)
\end{lstlisting}


Ein Ballobjekt wird also in der Liste \lstinline{balls} von Filter zu Filter 
gereicht um anschließend visualisiert zu werden (dies erledigt \lstinline{DrawBallsFilter}). 

Jedes Ballobjekt trägt dabei die Information über die eigene momentane Position und
- zu Visualisierungszwecken - auch eine Farbe und einen Radius-Wert. 

Bei jedem Aufruf von \lstinline{updatePosition} zum Aktualisieren der Position
an einer \lstinline{Ball} wird die bisherige Position in einer Liste gespeichert.
Jeder Ball trägt daher Information über seine bisherige Information, die in 
der Visualisierung auch als eine Folge von Linien hinter dem Ball dargestellt wird.

\begin{lstlisting}[language=Python,caption={\lstinline{Ball.py}, Ausschnitt}]
def updatePosition(self, position):
    self.positions = self.positions[:self.max_history] # keep positions list short
    self.positions.append(self.position)
    self.position = position
\end{lstlisting}


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{img/single-ball-position.png}
    \caption{Ein Ball mit den visualisierten Positionen in vergangenen Frames. Aktuelle Position: \lstinline{382/119}. Liste bisheriger Positionen: \lstinline{[(326, 206), (331, 185), (336, 172), (340, 156), (355, 120), (361, 114), (366, 110), (371, 110), (377, 114)]}}
\end{figure}

\section{Ansätze im Laufe des Projektes}
\label{sec:ansaetze}

Im Folgenden beschreiben wir die Arbeitsschritte, wie wir sie an Hand unserer
Grundplanung ausgeführt haben. Wir gehen dabei auf die Schwierigkeiten ein, auf die
wir getroffen sind, wodurch dann deutlich wird, weshalb wir zu unserer endgültigen
Umsetzung gekommen sind. Diese finale Umsetzung wurde bereits in Abschnitt \ref{sec:final} beschrieben.

\subsection{Anfängliche Programmstruktur}

Die ersten Tests, die wir mit der Kinect und bildverarbeitenden Verfahren
ausprobiert haben, bestanden aus wenigen Schritten und wurden in einer
simplen Schleife gelöst, die bei Druck der ESC Taste verlassen wurde. Zu Beginn
eines Schleifendurchlaufes wird ein neuer RGB Frame und die zugehörigen
Tiefendaten geholt. Danach folgt schrittweise Verarbeitung dieser Daten je nach
gewünschtem Zweck.


\begin{lstlisting}[language=Python, caption={Jeder Schleifendurchlauf entspricht einem Frame. Mit der SPACE Taste wird der aktuelle Frame als Bild gespeichert, jede andere Taste beendet die Schleife und somit das Programm.}]
    def loop(self):
        """ Start the loop which is terminated by hitting a random key. """
        while self.running:
            if self.record:
                self.snapshot()
            else:
                self._step()
            key = cv.WaitKey(5)
            self.running = key in (-1, 32)
            if key == 32: # space bar
                self.snapshot()

    def _step(self):
        """ One step of the loop, do not call on its own. Please. """
        # Get a fresh frame
        (rgb, depth) = self.kinect.get_frame()

        # ... normalize values

        # ... processing

        # transform to open CV representation
        rgb_opencv = cv.fromarray(np.array(rgb[:,:,::-1]))

        # Display image
        cv.ShowImage('display', rgb_opencv)

\end{lstlisting}



Nun möchten wir je nach betrachtetem Arbeitsschritt aber verschiedene
Verarbeitungsstufen ausführen oder ausklammern, so dass wir irgendwie eine
Parametrisierung finden mussten. Wir haben uns für ein Konzept von Filtern
entschieden, wobei zu Programmstart eine Liste mit gewünschten Filtern erstellt
wird und diese in der Hauptschleife nacheinander ausgeführt werden (die Originaldaten
werden also in den ersten Filter gegeben, das Ergebnis dieses Filters wird als
Eingabe für den zweiten Filter verwendet und das Ergebnis des letzten Filters
in der Liste wird dann als Gesamtausgabe visuell dargestellt.

Die RGB- und Tiefendaten werden vom freenect Modul als numpy Arrays zurückgegeben.
Die Ausgabe erfolgt über ein von OpenCV erzeugtes Fenster, welches Daten für
OpenCV erwartet. Die numpy Arrays müssen also an einer Stelle umgewandelt werden.
Zusätzlich haben wir schnell festgestellt, dass einige gewünschte Operationen
entweder nur in numpy oder nur in OpenCV zur Verfügung stehen. Ein mehrfaches
Umwandeln von einem Format in das jeweils andere ist aus Performanz-Gründen
natürlich zu vermeiden. Um in der Entwicklung nicht immer darauf achten zu müssen,
welcher Filter welche Eingabe erwartet und welche Ausgabe liefert, haben wir uns dazu
entschieden, jeden Filter als Eingabe Daten in numpy Darstellung zu liefern und auch
für die Ausgabe numpy zu erwarten. Dies erlaubt uns eine von außen identische
Betrachtung der Filter, auch wenn einige Filter intern eine Umwandlung durchführen
müssen. Bei etwaigen Problemen wollten wir die Performanz unserer Lösung getrennt
betrachten, um zu Beginn lediglich über eine funktionierende Lösung nachdenken
zu müssen.

Waren die Filter ursprünglich als isolierte Einheiten mit simplem
Input-Output-Verhalten von Bild- bzw. Tiefendaten gedacht, fiel uns schnell auf,
dass einige Filter Zusatzinformationen liefern, die von anderen Filtern gebraucht
werden. Um Filter nicht intern Unteraufrufe von anderen Filtern oder Komponenten
ausführen zu lassen (dies wäre ebenfalls eine denkbare Lösung), entschieden wir uns
für ein Objekt, das von der Hauptkomponente des Programms in jeden Filter
hineingereicht wird und in dem jeder Filter Informationen ablegen und abfragen kann.
Natürlich ist dies abhängig von der Reihenfolge der Filter und einige Filter haben
als implizite Vorbedingung, dass andere Filter bereits ausgeführt wurden, dies haben
wir der Einfachheit halber aber nicht expliziert formuliert, im Zweifelsfall wird
beim ersten Ausführen einer nicht korrekten Filterkombination oder -reihenfolge ein
Laufzeitfehler geworfen, da Informationen in dem übergebenen Objekt noch nicht
vorhanden sind. Dieses Objekt ist ein schlichtes Python dictionary.


{\color{red} FIXME: Diagramm Programmstruktur (so cool skizziert, nicht ganz formell UML)}

{\color{red} FIXME: bisschen Python Listing um diese Filter-Konzepte und das args dictionary zu zeigen}

Genau diese Problematik, dass die Abhängigkeiten von Filtern nur implizit festgelegt
ist und zur Laufzeit Fehler auftreten, weil eine benötigte Information im
\lstinline{args} dictionary nicht vorhanden ist, hat uns aber noch nicht zufrieden
gestellt. Daher haben wir das Filterkonzept weiter angepasst.

FIXME: Pointer finale Umsetzung

\subsection{Weitere Ansätze und Bildverarbeitungsverfahren}

\subsubsection{Temporale Ballerkennung}
\label{sec:temporal}

Ein alternativer Ansatz beruht auf der Idee, sämtliche sich nicht bewegende Teile im Tiefenbild herauszufiltern. Motiviert ist dies, um Teile der Vorerarbeitung des Tiefenbildes und der Ballerkennung flexibler zu gestalten. Bei der Vorverarbeitung wird, wie in \ref{sec:preprocess} beschrieben, davon ausgegangen, dass sich der Jongleur in einem bestimmten Abstand zum Kinect-Sensor befindet und seine Hände  die Bälle in einer bestimmte Distanz zu seinem Körper jonglieren. Zudem dürfen keine anderen Objekte,
beispielsweise Stühle, in der gleichen Ebene stehen und von der Kinect erfasst werden.
Der Temporale Filter entstand mit dem Wunsch, diese Einschränkung aufzuweichen und nach Möglichkeit komplett zu eliminieren.

Grundgedanke beim Temporalen Filter ist das Vergleichen von zwei zeitlich nah beieinanderliegenden Tiefenbildern.
Nur dort, wo sich Objekte in dem Zeitfenster zwischen den Aufnahmezeitpunkten der Tiefenbilder bewegt haben,
unterscheiden sich die Tiefendaten. Betrachtet man nur die Unterschiede, werden ausschließlich bewegte Objekte gefunden und sämtliche starre Objekte werden automatisch nicht mehr erfasst.
Zudem geschieht dies unabhängig von der Entfernung der Objekte zum Sensor.

Dabei gilt es aber zu bedenken, dass das Rauschen in den Tiefenbildern zu einem verrauschten Differenz-Bild führt. Zudem bewegt sich auch der Körper des Jongleurs leicht beim jonglieren. Das Verfahren hierbei muss also geeignete Maßnahmen implementieren, die diese Störeinflüsse kompensieren.

Da dieser Ansatz verschiedene Tiefenbilder miteinander vergleicht, muss dem Filter Zugriff auf Daten aus der Vergangenheit ermöglicht werden.
Zu diesem Zweck sammelt der Filter die Tiefenbilder intern in einem Buffer.
Nachdem zum ersten mal ein Bild im Buffer gespeichert wurde, kann im nächsten Filterdurchgang das momentane Tiefenbild mit dem vorherigen verglichen werden.
Bei der Implementation des Filters wird hierbei ein Differenzbild in Form eines Binärbildes erzeugt.
Für jeden Pixel wird überprüft, ob der Tiefenwert im alten Tiefenbild größer ist, als der entsprechende Pixel im aktuellen Tiefenbild.
Jenachdem wird entweder eine 1 oder eine 0 im Binärbild gespeichert.
Da bei dieser Betrachtung jeder Pixel unabhängig von den Werten in seiner unmittelbaren Umgebung verarbeitet wird, macht sich hierbei etwaiges Rauschen in den Tiefendaten besonders bemerkbar.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{img/temporal_scene.png}
    \caption{Die Differenz zwischen zwei Tiefenbildern in einem Binärbild dargestellt. Das Rauschen macht sich deutlich bemerkbar. Starke Differenzen treten im Bereich der Hände und der Bälle auf. Statische Objekte in der Umgebung, beispielsweise der Stuhl im Vordergrund, sind im Tiefenbild kaum erkennbar.}
    \label{temporal_rauschen}
\end{figure}

Um diese Fehlerhaften Pixel zu entfernen und falsch positive Ballpositionen zu verhindern, wird das Binärbild zunächst durch Erosion und anschließende Dilatation geglättet.
Die Bälle im Tiefenbild sind großflächig genug, so dass sie von diesem Vorgang nur bedingt beeinflusst werden.
Die Chance, durch die Erosion einen Ball komplett zu löschen ist daher nur sehr gering.
Im Gegensatz dazu verschwindet ein Großteil der verrauschten Pixel.
Diese bilden selten zusammenhängende Flächen und wenn, dann sind diese nur wenige Pixel groß.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{img/rauschfrei.png}
    \caption{Die Erosion und Dilatation het ein Großteil des Rauschens unterdrückt. Die Hand- und Ballbewegungen sind deutlich erkennbar.}
    \label{temporal_rauschfrei}
\end{figure}

Als Nebeneffekt werden hierbei auch einige Bereiche geglättet, in denen Körperbewegungen zu einer temporalen Veränderung im Tiefenbild geführt haben. Da sich der Körper im Vergleich zu den fliegenden Bällen nur mit einer sehr geringen Geschwindigkeit bewegt, entstehen hier auch nur schmalere Bewegungsschatten im Binärbild.
Nach dieser ersten Glättung des Differenzbildes, werden zusammenhängende Bereiche extrahiert.
Unter der Annahme, dass die Bälle im Binärbild eine Fläche bilden, die eine Mindestanzahl an Pixeln nicht unterschreitet, findet anschließend eine erneute Überprüfung statt. Alle zusammenhängende Bereiche, die in der Summe zu wenig Pixel haben, werden ignoriert.
Nur die Bereiche, die groß genug sind, werden gesammelt und als potientelle Bälle in einer Liste gespeichert.
Diese Liste wird anschließend in der Filterkette weitergerreicht und bildet somit das Ergebnis des temporalen Ballerkennnugsfilters.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{img/temporal_final.png}
    \caption{Ein Beispiel für vom temporalen Filter gefundene Ballpositionen.}
    \label{temporal_final}
\end{figure}

Große Auswirkung auf die Güte der Ballpositionen hat hierbei der zeitliche Abstand der beiden Tiefenbilder, die zu Beginn verglichen werden. Wenn die Bilder zeitlich so nah beieinander liegen, dass sich die Ballflächen in den beiden Frames teilweise überschneiden, entstehen halbmondförmige Flächen im Binärbild.
Diese werden dann aber unter Umständen von dem Erosions-Prozess vernichtet.
Oder aber in der Pixelsumme so stark reduziert, dass sie in der anschließenden Überprüfung aussortiert werden.
Gleiches kann auch im Scheitepunkt der Wurfparabel auftreten. Auch bei größeren zeitlichen Abständen besitzen die Bälle an dieser Stelle eine zu geringe vertikale Geschwindigkeit um sich in aufeinander folgenden Bildern nicht zu überschneiden.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.7]{img/halbmond.png}
    \caption{In Situationen, in denen sich der Ball nicht schnell genug bewegt oder die Tiefenbilder zeitlich zu nah aufeinander folgen, löscht sich der Ball im Differenzbild teilweise selbst aus und es entstehen halbmondförmige Flächen. }
    \label{temporal_halbmond}
\end{figure}

Auf der anderen Seite führt ein zeitlich zu großer Abstand zwischen den zwei Tiefenbildern dazu, dass auch langsame Bewegungen des Jongleurs zu großen Bewegungsschatten führen, die im Folgenden nicht mehr herausgefiltert werden können.

Zur Erosion, Dilatation und zum Finden von zusammenhängenden Bereichen im Binärbild kam hierbei die Bildbearbeitungsbibliothek VIGRA \cite{vigra} zum Einsatz.
Dies ermöglicht eine effiziente Durchführung der entsprechenden Operationen.
Dennoch machen sich die zusätzlichen Berechnungen in der Framerate bemerkbar und erschweren die Echtzeitfähigkeit der Ballerkennung. Gepaart mit der teilweise fehlerhaften Ballerkennung,
bedingt durch die Selbst-Überdeckung der Bälle in den langsameren Teilen der Wurfbewegung, wurde dieser Ansatz letztendlich für die endgültige Lösung verworfen und dementsprechend auch nicht weiter ausgearbeitet.

\subsubsection{Kalman Filter}

Der Kalman Filter ist ein in 1960 von Rudolf E. Kálmán vorgestellter Algorithmus, welcher aus mehreren ungenauen Messungen versucht, die tatsächlichen Daten, die diesen Messungen zu Grunde liegen, zu schätzen.\cite{kalman} Ungenauigkeiten können durch die Messsensoren entstehen, wenn diese aus verschiedenen Gründen nicht präzise genug aufnehmen können, oder auch durch ein Rauschen der Aufnahmequelle. Mit dem Kalman-Filter können so über einen bestimmten Zeitraum aufgenommene Daten mit möglichst vielem Wissen über potentielle Messabweichungen genauer bestimmt werden, als mit nur einzelnen Messungen.

Dieser Algorithmus wird heutzutage in vielen Bereichen eingesetzt, sehr oft zum Beispiel in der Navigationstechnologie von Fahrzeugen aller Art. Die Position eines Fahrzeuges wird beispielsweise über GPS empfangen. Diese Technologie kann die Position nur auf ca. 5-20m genau bestimmen. Selbst mit einer zusätzlichen Korrektur (WAAS/EGNOS-Korrektursignale mit Hilfe von Bodenstationen\cite{waas}\cite{egnos}) erreicht die Genauigkeit nur etwa 1-3m. Unter umständen wird eine exaktere Positionsbestimmung nötig, zum Beispiel bei Fortbewegungsmittel, die ein automatisches Fahrsystem verbaut haben.

So können weitere Sensoren verbaut werden, die zusammen mit dem GPS eine genauere Positions- und Bewegungsbestimmung ermöglichen, aber selbst bei diesen können erneut Messungenauigkeiten auftreten. Mit dem Kalman Filter können so alle aufgenommene Daten mit dem Wissen über die Art der Bewegung und Annahmen zu Messfehlern verfeinert werden. Zusätzlich kann der Algorithmus mit den bisher gesammelten Daten eine Schätzung über die Position in der Zukunft machen. Wie erwähnt nutzt dieser Algorithmus alle bisher aufgenommenen Daten, jedoch werden diese als Markow-Kette modeliert, so dass eine interne Formel immer mit den neusten Daten angepasst wird und keine Iteration über alle vergangenen Daten notwendig ist. Dadurch können diese Berechnungen in Echtzeit erfolgen, was sie so wertvoll macht.

Eine einfache Implementierung dieses Kalman Filters besitzt zwei Funktionen: ein {\tt predict} und ein {\tt update}.

Im {\tt predict} wird mit Informationen zur Art der Bewegung und mit mindestens einer Ortsinformation zum Zeitpunkt {\tt t-1} eine Schätzung zum Ort im Zeitpunkt {\tt t} gemacht. Zusätzlich werden hierbei die Sensordaten im Zeitpunkt {\tt t} geschätzt. Ein {\tt predict} kann beliebig oft hintereinander ausgeführt werden.
\begin{align}
\overline{x}_{t} = A_{t}x_{t-1} + B_{t}u_{t} + \varepsilon_{t} \label{eq:predict1}\\
\overline{z}_{t} = H_{t}\overline{x}_{t} + \varepsilon_{t} \label{eq:predict2}
\end{align}

In Formel \eqref{eq:predict1} wird die voraussichtliche Position zum Zeitpunkt {\tt t}, $\overline{x}_{t}$, berechnet. Hierbei ist $A_{t}$ das Bewegungsmodell, das auf die Position zum Zeitpunk {\tt t-1} angewandt wird, also bei einem Kraftfahrzeug wäre dies die Geschwindigkeit und Richtung im Zeitabschnitt {\tt t-1} bis {\tt t}, welche auf die vorige Position des Fahrzeuges gerechnet werden, um die aktuelle Position zu bestimmen.\\
$u_{t}$ ist ein Vektor, der bekannte Störungen darstellt, multipliziert mit einer Matrix $B_{t}$, welche dynamisch angepasst wird.\\
$\varepsilon_{t}$ ist noch zusätzliches Rauschen, welche mit einer Normalverteilung einbezogen werden.

In Formel \eqref{eq:predict2} wird die Ausgabe der Sensoren zum Zeitpunkt {\tt t}, $\overline{z}_{t}$, geschätzt. Es wird hierbei die geschätzte Position $\overline{x}_{t}$ mit einer Beobachtungsmatrix $H_{t}$ multipliziert, welche die Abweichung von tatsächlicher Position zu beobachteter Position beinhaltet. Zusätzlich wird hier ebenfalls ein Rauschen in Form einer Normalverteilung hinzuaddiert.

Im {\tt update} wird die aktuell gespeicherte Ortsinformation mit den extern gewonnen Sensordaten aktualisiert. Die hier gemessene Abweichung zwischen Schätzung und tatsächlichen Daten kann natürlich auch zur weiteren Schätzung eingebracht werden. Ein {\tt update} wird nicht mehrmals hintereinander ausgeführt.
\begin{align}
x_{estimate} = \overline{x}_{t} + K(z_{t} - \overline{z}_{t}) \label{eq:update}
\end{align}

Ein {\tt update} ist in Formel \eqref{eq:update} dargestellt. Mit der Abweichung zwischen den neuen Sensordaten $z_{t}$ und den geschätzten Sensordaten $\overline{z}_{t}$ wird die vorher geschätzte Position zum Zeitpunkt {\tt t}, $\overline{x}_{t}$, angepasst, so dass die angenommene Position $x_{estimate}$ errechnet wird.

Einem {\tt update} geht immer ein {\tt predict} voran, jedoch kann ein {\tt predict} beliebig oft, mit neuen Zeiten, hintereinander ausgeführt werden. Hierdurch werden Schätzungen über die Position des beobachteten Objekts zu weiteren Zeiten berechnet, deren Genauigkeit, abhängig von Art der Bewegung, meist mit größerer Zeit sinkt. Jedoch können diese bei ausreichendem Wissen über Bewegung und Störungen für eine gewisse Zeit ausreichend genau sein, um mit ihnen zu arbeiten.

Ursprünglich sollte der Kalman-Filter bzw. eine Vereinfachung dessen in diesem Projekt dazu verwendet werden, durch Beobachtung der einzelnen Bälle die voraussichtlichen Flugbahnen dieser zu bestimmen. Um diese Flugbahnen abzuschätzen, muss jedoch jeder Ball über mehrere aufeinander folgende Frames hinweg verfolgt werden können. Es reicht also nicht, in jedem Frame alle Bälle zu erkennen, sondern es muss erkannt werden, wie sich die Ballposition von jedem Ball von einem Frame zum nächsten verändert hat. Genau dafür kann nun der Kalman-Filter ebenfalls genutzt werden. Durch Schätzen der Ballposition im nächsten Frame wird die Zuordnung der Bälle unterstützt.

So ist in diesem Projekt das bewegte Objekt ein Ball und die Art der Bewegung eine Wurfparabel. Der Einfachkeit halber und da in diesen Zeiten vorkommende Abweichungen klein wären, außerdem eine Genauigkeit auf Pixelebene nicht notwendig ist, sind keine Störfaktoren einberechnet worden.

In den ersten Implementationen dieses Ansatzes hatten wir einen Bug verbaut, welcher erst sehr viel später bemerkt wurde. Durch diesen funktionierte die Schätzung der Ballposition in nachfolgenden Frames nicht, da die zwei Startpositionen eines Balles, mit dem der weitere Wurf geschätzt wurde, aus demselben Frame genommen wurden. Auf der Suche nach dem Fehler schrieben wir die Handerkennung, um die Handpositionen aus den möglichen Ballpositionen herauszunehmen. Im Zuge dessen wurde der Bug unbemerkt korrigiert, wodurch der darauf folgende Erfolg dem Entfernen der Handpositionen und einer einfacheren Schätzung zugeschrieben wurde, in der Ballpositionen in aufeinander folgenden Frames durch eine möglichst kurze Entfernung zugeordnet werden. Da dieser vereinfachte Ansatz 

\subsubsection{Hough-Transformation}
\label{sec:hough}
{\color{red} FIXME ROLF Python Code-Beispiel}
Die Hough-Transform wurde in ihrer heutigen Form 1972 von Richard O. Duda und Peter E. Hart\cite{hough} vorgestellt.\\
Ursprünglich wurde das Verfahren zum Finden von geraden Linien von Hough patentiert, jedoch dann von Duda und Hart verbessert und auch das Finden von Kreisen bzw. Kurven ermöglicht.

Das Finden von Geraden wird so erreicht, dass für jeden Kantenpunkt (die vorher mit einem Kantendetektor, wie z.B. dem Canny-Algorithmus, gefunden werden) alle möglichen Geraden, die durch diesen Punkt gehen, aufgestellt werden, und von diesen die Parameter im Parameterraum abgebildet werden.\\
Anfangs wurden die Steigung und der y-Achsenabschnitt verwendet, doch da sich Probleme bei Kanten parallel zur y-Achse ergaben, wird nun die Hessesche Normalform der Geraden verwendet und als Parameter ein Winkel $\alpha$, in welchem ein Vektor vom Koordinatenursprung orthogonal eine Gerade schneidet, die durch den zu betrachtenden Punkt geht, verwendet. Der weitere Parameter ist die Länge dieses Vektors, bzw. der Abstand vom Ursprung bis zur Geraden, die orthogonal unseren Hilfsvektor schneidet.

\begin{figure}[H]
  \centering
      \includegraphics[scale=1.1]{img/Hough1.png}
  \caption{Anstatt Steigung und y-Achsenabschnitt werden als Parameter $\alpha$ und d genommen.}
\end{figure}
Wenn dies nun für alle vorhandene Kantenpunkte gemacht wird, werden im Parameterraum eine dementsprechende Anzahl von Kurven (oder Geraden) abgebildet, die einen exakten, oder ungefähren Schnittpunkt besitzen sollten. Die Parameter, die an diesem Punkt abgelesen werden, stellen die Parameter für unsere Kante im Bild durch die Kantenpunkte dar.\\\\
\begin{figure}[H]
  \centering
      \includegraphics[scale=1.1]{img/Hough2.png}
  \caption{Überschneidung der Kurven im Parameterraum geben an, welche Parameter für die gesuchte Gerade zu nutzen sind.}
\end{figure}
Möchte man nun einen Kreis im Bild finden, werden von den Kantenpunkten alle möglichen Kreisgleichungen aufgestellt, dementsprechend als Parameter der Radius und der Mittelpunkt $(x,y)$ gewählt, womit wir einen dreidimensionalen Parameterraum haben.\\
Unter berücksichtigung eines Maximalwertes für den Radius tut man dies für alle Kantenpunkte, und anschließend lässt sich im Parameterraum wieder ein Schnittpunkt ablesen, an der sich die Parameter des gesuchten Kreises befinden.\\\\
\begin{figure}[H]
  \centering
      \includegraphics[scale=1.1]{img/Hough3.png}
  \caption{Es entstehen Kegeln, an denen sich wieder ein Schnittpunkt bildet.}
\end{figure}
Dieses kann man für beliebige Formen anwenden, die durch eine Parameterdarstellung definiert werden können, aber je mehr Parameter vorhanden sind, desto aufwändiger wird auch die Berechnung werden.

Dieses Verfahren sollte genutzt werden, um die genauen Ballpositionen im Tiefen- oder auch RGB-Bild zu bestimmen. Wenn möglich, sollte nach dem Filtern nach Regions of Interest im Tiefenbild auf diesen gefundenen Regionen die Hough-Transformation durchgeführt werden, um Kreise bzw. Bälle zu finden, oder auch Regionen ohne Bälle ausschließen zu können.\\
Genutzt haben wir die Implementation der Hough-Transformation in der OpenCV\cite{opencv}-Bibliothek. Hierbei wurde festgestellt, dass durch Einstellen der Parameter der Funktion die Kreiserkennung sehr fein bestimmt werden kann. Da, wie in Abbildung \ref{bewegungsunschaerfe} zu sehen ist, die Bälle fast nie als saubere Kreise mit scharfen Kanten aufgenommen werden, war das Feintuning der Parameter unmöglich. Entweder wurde sehr grob eingestellt, so dass alle Bälle zwar erkannt wurden, dabei aber noch viel mehr False-Positives entstanden, oder es wurde so fein eingestellt, dass nur sehr sauber aufgenomme Bälle erkannt wurden. Bei letzterem wurden insgesamt aber viel zu wenige Bälle gefunden, so dass durch Einsetzen dieses Verfahren in unserem Fall kein Vorteil, sondern nur Nachteile entstanden.\\
Zudem wurde festgestellt, dass dieser Algorithmus viel Rechenleistung beansprucht und durch viel Kreisdetektion die ausgegebene Frame-Rate sank.

Die Überlegung war, dass durch eine Einbindung des Algorithmus als C-Code, z.B. mit Cython\cite{cython}, dieser Leistungsverlust ausgeglichen werden könnte. Durch eine eigene Implementation hätte die Ballerkennung evtl. auch verbessert werden können, so dass nicht nach Kreisen gesucht wird, sondern nach Ellipsen (in bestimmter Ausrichtung). Da aber die ersten Versuche nicht sonderlich erfolgreich waren und der Aufwand nicht lohnenswert erschien, wurde dagegen entschieden. Durch geschicktes filtern nach unseren Regions of Interest konnten diese auch soweit verkleinert werden, dass eine zusätzliche Beschränkung dieser nicht notwendig wurde. Auch das Entfernen von False-Positives in den Regions of Interest wurde auf anderem Wege gelöst.

\subsubsection{Local Maxima}
Mit Local Maxima werden lokale Maxima, also positive extreme Datenpunkte in einer lokal begrenzten Umgebung, bezeichnet.\\
Anfangs hatten wir die Idee, über lokale Maxima die Bälle im Tiefenbild zu finden. Dies würde bedeuten, dass im Tiefenbild nach allen Datenpunktanhäufungen gesucht wird, die möglichst nahe der Kamera sind, dabei aber innerhalb eines gegebenen Radius bleiben. Diese sollen zusätzlich von Datenpunkten umgeben sein, die einen deutlichen Abstand nach hinten zu diesen Datenpunktanhäufungen besitzen.\\
Umgangssprachlich würden wir also alle Objekte bestimmter Größe suchen, die sich in einem bestimmten Abstand zu ihrem Hintergrund befinden. Damit ließe sich ein flexibler Abstand von Jongleur zu Kinect bewerkstelligen, es könnte damit sogar eine Ortsveränderung des Spielers während der Jonglage verarbeiten.

In unserem Projekt werden die Tiefendaten zumeist als Numpy-Arrays verarbeitet und von Filter zu Filter gereicht. So war hier die Idee, einen weiteren Filter zu erstellen, in dem im Numpy-Array nach lokalen Maxima bestimmter Größe gesucht wird, um eine Vorauswahl von Regions of Interest zu bekommen. In diesen sollten in weiteren Schritten False Positives ausgeschlossen werden, um am Ende nur Regionen mit Bällen zu erhalten, in denen mit z.B. der Hough-Transformation (zu diesem Zeitpunkt wollten wir diese noch nutzen) die Bälle extrahiert werden sollten.

Für das Finden von Maxima im Numpy-Array werden die Python Module Numpy\cite{numpy} und Scipy\cite{scipy} genutzt, welche eine große Auswahl an wissenschaftliche Funktionen besitzen. Besonders für mehrdimensionale Matrixberechnungen und weitere Bildverarbeitungsoperationen eignen sich diese Module sehr gut, da ihre Funktionen auf diese Probleme optimiert sind und meist schneller als eine eigene Implementierung sind.

Speziell mit den Funktionen {\tt scipy.ndimage.filters.maximum\_filter} und\\ {\tt scipy.ndimage.filters.minimum\_filter} wurden auf einem Tiefenbild die Maxima und Minima bis zu einem festgelegten Durchmesser gesucht. Durch Abziehen der Minima von den Maxima konnte auf dem daraus resultierenden Array mit einem Schwellwert, welcher den Abstand eines Maxima zu seiner Umgebung beschreibt, lokale Maxima bestimmter Größe gefunden werden.

Dieses Verfahren, welches bei Testbildern, welche rauscharm mit einem gleichmäßigen Hintergrund sind, auf dem deutlich hellere Punkte verteilt sind, funktioniert gut. In unserem Falle aber bestand das Problem, dass bei verschiedenen Einstellungen der Parameter entweder zu viele Maxima, wie beispielsweise jede sich hervorhebende Falte in der Bekleidung des Spielers gefunden wird, oder aber zu wenige, so dass sich nicht alle Bälle immer in den Regions of Interest befinden würden.\\
So haben wir uns entschieden, keinen flexiblen Abstand des Spielers zur Kinect zuzulassen, sondern diesen festzulegen. Anfangs als Nachteil empfunden, stellten wir aber fest, dass dies schnell akzeptiert werden konnte und den Spielspaß nicht mindert.

Unsere finale Umsetzung zum Finden von Regions of Interest wird in Abschnitt \ref{sec:roi} beschrieben.

\subsection{Ballerkennung}

Bevor wir die einzelnen ausprobierten Ansätze zur Ballerkennung erläutern, ist kurz
der Weg zu beschreiben, auf dem wir zum Endergebnis gekommen sind. Das Endergebnis
ist dabei tatsächlich simpel, relativ robust und in
wenig Codezeilen implementiert.

Den entscheidenden Fortschritt haben wir erzielt, als wir endlich bemerkt haben,
dass das simple Erkennen der Hände und das Entfernen der Handpositionen aus der
Liste der zu analysierenden potentiellen Ballpositionen die Aufgabe sehr
vereinfacht.

Bis wir diesen Schritt aber gegangen sind, haben wir etliche immer komplexere
Ansätze über die Vorhersage möglicher Ballpositionen durch Simulation des
physikalischen Verhaltens eines Jonglierballs während des Wurfes verfolgt.

Bevor wir also die Handerkennung als Teil der Ballerkennung betrachtet haben,
haben wir etliche Fehlerquellen mehr gehabt, die die Aufgabe der Ballerkennung
frustrierend schwer gemacht haben. Die folgenden Seiten berichten von unseren
Ansätzen und unbefriedigenden Ansätzen in dieser Projektphase.

\subsubsection{Fehlerquellen}

Als erste Idee, um falsche Positionen auszuschließen, dachten wir an eine
Ballerkennung auf den RGB-Daten im Umfeld der Position. Naheliegend war die die
Implementierung einer Hough-Transformation (siehe Abschnitt \ref{sec:hough}), mit der wir auch begonnen haben.

Erste Testläufe auf den RGB-Daten der Kinect zeigten aber, dass je nach Wahl der
Parameter entweder die Bälle nicht erkannt, oder aber viel zu viele false-positives
erkannt wurden. Dies lag vor allem an den sehr verwischten Aufnahmen der Bälle
während des Fluges. Bei Betrachtung der Beispielaufnahmen (siehe oben) wird auch
deutlich, dass eine vermutete Position teilweise deutlich von der tatsächlichen
Position in den Bilddaten abwich. Somit konnten wir durch diesen Ansatz keinen Gewinn
ziehen, so dass wir uns auch durch die Abweichungen zwischen Ballposition in RGB- und
Tiefendaten dazu entschlossen, uns alleine auf eine Untersuchung der Tiefendaten zu
stützen.


\begin{figure}[H]
  \centering
      \includegraphics[scale=0.8]{img/rgb-blur.png}
  \caption{Verdeutlichung der abweichenden Bewegungsunschärfe in den RGB-Daten. Die Positionen der Bälle sind hier mit roten Markierungen hervorgehoben (nachträglich per Hand eingefügt)}
  \label{bewegungsunschaerfe}
\end{figure}


Aber auch für die reine Berücksichtigung der Tiefendaten haben wir während des
Projektablaufes mehrere Problemquellen identifizieren können, die die verschiedenen
Ansätze unterschiedlich stark beeinflussen:

\begin{itemize}
 \item Die Hände sind auch in den möglichen Ballpositionen als Rechtecke enthalten. Dass dies die entscheidende Fehlerquelle ist, die wir mit einem simplem Verfahren ausschließen können (siehe FIXME), ist uns wie oben beschrieben erst spät aufgefallen.
 \item Die Bälle fliegen sehr nahe beieinander und teilweise überschneiden sich die Rechtecke zweier Bälle, so dass nur ein großes Rechteck zu sehen ist und als eine mögliche Ballposition untersucht wird.
 \item Ein Ball legt in einem Frame (1/30 Sekunde) unterschiedlich lange, teilweise sehr große Strecken zurück.
 \item Der Mindestabstand zur Kinect resultiert in einem kleinen Jongliermuster, wodurch die problematischen Faktoren verstärkt werden.
 \item In aufeinander folgenden Frames wird zum Teil eine Region in einigen der Frames nicht erkannt, in anderen schon. Diese "Lücken" in der Flugbahn haben bei unseren ersten Ansätze große Probleme bereitet, da plötzlich nicht mehr genug potentielle Ballpositionen für unsere bereits instantiierten Ballobjekte zur Verfügung stehen. Im finalen Ansatz haben wir solche Ballobjekte dann verworfen und
 bei Bedarf dynamisch neue Bälle erzeugt.
\end{itemize}

\subsubsection{Ansätze}

Die genannten Problemquellen wirken sich unterschiedlich stark auf die Eignung der von
uns im folgenden betrachteten Ansätze aus. Prinzipiell lassen sich die Ansätze
in zwei Gruppen unterteilen: In die erste Gruppe, bei der von vorneherein von einer
festen Ballanzahl ausgegangen wird und die zweite Gruppe, bei der auch eine dynamisch wechselnde Ballanzahl erlaubt ist.

Ursprünglich sind wir davon ausgegangen, dass die Grundannahme, dass genau drei
Bälle in der Jonglage vorhanden sind, uns bei der Analyse hilft, da wir besser
ausschließen können, welche Position kein Ball sein kann. Dies stellte sich als
Fehlannahme heraus und ist in der finalen Version so auch nicht mehr in der
Ballerkennung implementiert.

Bei den Ansätzen mit fester Ballanzahl wird in jedem Verarbeitungsschritt versucht,
zu jeder bereits existierenden Ballinstanz eine Position aus den erkannten Rechtecken
auszuwählen, welche als neue Position des Balls festgelegt wird. Da im Allgemeinen nie
zwei Bälle an der gleichen Position sein können, wird dieses Rechteck dann für die weiteren Ballinstanzen nicht mehr betrachtet.

 \begin{enumerate}
 \item Die zwei sich am nächsten Punkte in zwei aufeinander folgenden Frames werden als der identische Ball aufgefasst. Nicht so zuverlässig, vor allem wegen schneller Ballbewegungen und nahe aneinander passierender Bälle. Schwierig auch, wenn ein erkannter Ball fehlt $\rightarrow$ Beachtung von ``springenden'' Bällen.
 \item Verbesserungsansatz: Die erwartete Ballposition wird mit der vorherigen Bewegung (linearer Bewegungsvektor) approximiert. Dies ist teilweise besser, aber trotzdem noch schwierig, die initiale Bewegung zu Erkennen. Hierbei verbleiben auch weiterhin Probleme mit Lücken in den Informationen.
 \item Weitere Verbesserung: Keinen linearen Bewegungsvektor nutzen, sondern die Flugbahn vorberechnen. Der lineare Bewegungsvektor wird als Tangente an der Steigung der Wurfparabel zu Grunde gelegt. In unseren Tests hat dies aber auch nur zu einer geringfügigen Verbesserung beigetragen.
 \end{enumerate}

Im Gegensatz dazu sind die folgenden Ansätze zu sehen, bei denen wir auch eine variable Ballanzahl erlauben. Hierbei werden nicht wie im vorigen Ansatz mögliche Ballpositionen nach einer Zuweisung entfernt.

\begin{enumerate}
	\item Wenn eine langsame Aufwärtsbewegung in aufeinander folgenden Frames erkannt wird: als Beginn eines Wurfes auffassen und an dieser Stelle einen Ball mit identischer Geschwindigkeit starten und dessen Flugbahn ab dort schrittweise simulieren. In jedem Schritt mit aktuell vorhandenen Bällen abgleichen und Wurfparameter anpassen. (hier schrittweise Bilder zeigen: Feuerwerk etc)
	\item Wie der Ansatz davor, allerdings wird die Parabel nicht approximiert, sondern	aus den letzten 3 Frames mit \lstinline{np.polyfit} berechnet.
\end{enumerate}

Für diese Ballzuweisung wird es notwendig, mindestens den nächsten Abschnitt der Flugbahn eines Balles, optimalerweise aber auch die gesamte Flugbahn eines Balles zu approximieren.\\
In den ersten Versuchen, den Wurf zu modellieren, wurde die Idee von Kalman aufgegriffen und auf sehr einfache Weise versucht, diese umzusetzen. Dazu wurde erst einmal die Bewegung des Balls als eine Wurfparabel festgelegt, dessen mathematische Formel wie folgend ist:
\begin{align}
x& = v_{x}t \label{eq:wurf1}\\
y& = v_{0_{y}t} - \frac{g}{2}t^{2} \label{eq:wurf2}
\end{align}
Die Bewegung in der x-Achse, also der Horizontalen, ist eine Bewegung mit konstanter Geschwindigkeit $v$ über die Zeit $t$. In der Vertikalen y-Achse wirkt die Schwerkraft mit konstanter Beschleunigung $g=9,81 \frac{m}{s^{2}}$ nach unten auf den Ball ein. Hiermit lässt sich eine Position des Balls bei Zeitpunkt $t$ bestimmen, wenn Anfangsposition und Geschwindigkeiten in beide Richtungen bekannt sind. In unserem Fall kann eine Anfangsposition bestimmt werden, jedoch ist aus einem Frame allein die Geschwindigkeit nicht zu bestimmen. Werden mindestens zwei Frames genutzt, und in diesen Bälle gefunden, die einander zugeordnet werden, kann hieraus eine Anfangsposition und zusätzlich eine Richtung sowie Geschwindigkeit gewonnen werden. Da nun anstatt der Geschwindigkeit in horizontaler und vertikaler Richtung nur eine allgemeine Geschwindigkeit des Balles, aber zusätzlich eine Richtung extrahiert werden konnte, werden folgende Formeln für den Ballwurf genutzt:
\begin{align}
x(t)& = v_{0}t\cos\beta \\
y(t)& = v_{0}t\sin\beta - \frac{g}{2}t^{2}
\end{align}
Mit $v_{0}$, der Geschwindigkeit des Balles und $\beta$, dem Winkel, in dem der Ball schräg nach oben geworfen wird, kann nun intern der Wurf modelliert werden.

Ein Beispiel einer einfachen Implementierung dessen mit zwei vorgegebenen Ballpositionen:

\begin{lstlisting}[language=Python]
class SimpleKalmanFilter(object):

    def trajectory(self, (y1, x1), (y2, x2), t):
        b = np.degrees(np.arctan((y2-y1)/(x2-x1)))  # Winkel, in dem hochgeworfen wird
        v = np.sqrt((x1 + x2)**2 + (y1 + y2)**2)    # Geschwindigkeit des Balls
        x = v * t * np.degrees(np.cos(b))
        y = v * t * np.degrees(np.sin(b) - (9.81/2) * t**2)
        return (y,x)

    def filter(self, rgb, depth, balls, args = {}):
        p1 = (480, 0)
        p2 = (470, 10)
        points = []

        for t in xrange(1,10):
            point = self.trajectory(p1, p2, t)
            points.append(point)

        return points
\end{lstlisting}
Hierbei wird durch Aufruf von {\tt SimpleKalmanFilter.filter} für zwei aufeinanderfolgenden Ballpositionen in einer Schleife für 10 Zeiteinheiten eine voraussichtliche Wurfbahn berechnet und zurückgegeben.

In der Theorie sollte dieser Ansatz funktionieren, aber in für uns unerklärbarer Weise wurden im Praxisversuch die Wurfbahnen teils wie erwartet, teils aber ganz anders eingezeichnet. So haben wir in weiteren Versuchen die Berechnung vereinfacht, und uns an Formeln \ref{eq:wurf1} und \ref{eq:wurf2} gehalten:

\begin{lstlisting}[language=Python]
class TrajectoryBall(object):
    def __init__(self, lowerPoint, upperPoint, meta=None):
        [...]
        # gravity, positive because y is upside-down
        self.gravity = 9.81 * 0.5
        self._initTrajectory(lowerPoint, upperPoint)

    def _initTrajectory(self, lowerPoint, upperPoint):
        (x1, y1), (x2, y2) = lowerPoint, upperPoint

        # speed in x and y direction
        self.v_x = x2 - x1
        self.v_y = y2 - y1

        # start trajectory at first point we're based on
        self.xOffset = x1
        self.yOffset = y1

        # progress of the trajectory
        self.t = 0

    def _trajectory(self, t=1):
        """Calculate the throw trajectory based on 2 points to return any future or past point on that trajectory"""

        # distance in x and y direction
        x   = self.v_x * t + self.xOffset
        y   = self.v_y * t + self.gravity/2 * t**2 + self.yOffset

        return int(x), int(y)
\end{lstlisting}
Dieser Ansatz brachte leichte Verbesserungen ein, jedoch wurden viele der Wurfbahnen trotzdem noch in unseren Augen willkürlich eingezeichnet, so dass Würfe, die in eine Richtung gehen, gerne mal spiegelverkehrt oder gänzlich aus dem Bild fliegend eingezeichnet wurden.

\begin{comment}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/trajectory-1.png}
    \vspace{-0.5cm}
    \caption{Simulierte Flugbahnen zur Vorhersage tatsächlicher Ballposition.}
    \label{ballcount-1}
\end{figure}

%\begin{figure}[H]
%    \centering
%    \includegraphics[scale=0.5]{img/trajectory-2.png}
%    \vspace{-0.5cm}
%    \caption{Simulierte Flugbahnen zur Vorhersage tatsächlicher Ballposition.}
%    \label{ballcount-1}
%\end{figure}
%\end{comment}

%\begin{figure}[H]
%    \centering
%    \includegraphics[scale=0.5]{img/trajectory-3.png}
%    \vspace{-0.5cm}
%    \caption{Simulierte Flugbahnen zur Vorhersage tatsächlicher Ballposition.}
%    \label{ballcount-1}
%\end{figure}



\begin{tabular}{*{2}{m{0.7\textwidth}}}
Gut: Genaue Approximation tatsächlicher Ballpositionen auf vielen der simulierten Flugbahnen. & \includegraphics[scale=1]{img/trajectory-good.png} \\
Schlecht: Viele simulierte Positionen mit Teils großen Abweichungen. Automatisiert ist es schwer, nur die guten Positionen zu erkennen und zur tatsächlichen Ballerkennung zu nutzen. & \includegraphics[scale=1]{img/trajectory-bad.png}
\end{tabular}


Wie sich herausstellte, waren die Handpositionen, die immer in den potentiellen Ballpositionen mit aufgenommen waren, ein Hauptgrund für das fehlerhafte Verhalten. Wurden die Hände erst einmal separat erkannt und aus den potentiellen Ballpositionen entfernt, fallen bereits viele der fälschlich simulierten Flugbahnen heraus.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/trajectory-hand-1.png}
    \vspace{-0.5cm}
    \caption{Simulierte Flugbahnen zur Vorhersage tatsächlicher Ballposition, Positionen der Hände wurden herausgerechnet und nicht als fälschliche Grundlage für Flugbahnen verwendet.}
    \label{ballcount-1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/trajectory-hand-2.png}
    \vspace{-0.5cm}
    \caption{Simulierte Flugbahnen zur Vorhersage tatsächlicher Ballposition, Positionen der Hände wurden herausgerechnet und nicht als fälschliche Grundlage für Flugbahnen verwendet.}
    \label{ballcount-1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/trajectory-hand-3.png}
    \vspace{-0.5cm}
    \caption{Simulierte Flugbahnen zur Vorhersage tatsächlicher Ballposition, Positionen der Hände wurden herausgerechnet und nicht als fälschliche Grundlage für Flugbahnen verwendet.}
    \label{ballcount-1}
\end{figure}

Sobald wir aber die Hände mit in die Betrachtung eingezogen haben, war das Simulieren der Flugbahnen gar nicht mehr nötig. Der ganze Vorgang der Zuordnung von bereits initiierten Ballobjekten zu potentiellen Positionen konnte nochmals vereinfacht werden und und läuft im finalen Ergebnis sehr gut, wie in Abschnitt \ref{sec:ballzuordnung} beschrieben.




\section{Herausforderungen}
Während dieses Projektes wurden viele Ideen eingebracht, einige sofort implementiert, andere nach mehreren Versuchen nutzbar gemacht. Aber einige Ansätze konnten auch nach wiederholten Versuchen nicht ausreichend umgesetzt werden, oder schienen nach mehrmaligem Testen nicht den erhofften Fortschritt hineinzubringen, weswegen sie letzten Endes nicht in der Software genutzt werden.

Einige dieser Ansätze, die uns Schwierigkeiten bereitet haben, werden hier kurz vorgestellt, das Problem der Idee aufgezeigt, und wie wir sie letzten Endes doch implementiert haben - oder warum sie es nicht in die Software schafften.\\
Leider machten wir während des Projektes wenige Aufzeichnungen über unsere Arbeitsschritte, sondern programmierten wild darauf los, und was nicht funktionierte, wurde überschrieben. Deswegen ein Reminder, vor allem an uns: Mehr Kommentieren und Fortschritt sowie Missglücktes notieren!


\subsection{Technische Herausforderungen}

Im Laufe des gesamten Projektes haben wir drei Projektteilnehmer verteilt auf drei
Betriebssystemen gearbeitet (Mac OS, Windows, Ubuntu). Da wir gerade dank
unseres flexiblen Filterkonzeptes gut arbeitsteilig arbeiten konnten, haben wir
nicht von Anfang an auf die Integrierbarkeit unserer Komponenten geachtet.

Der funktionierende Temporalfilter (siehe \ref{sec:temporal}) etwa wurde mit Hilfe
der Bildverarbeitungsbibliothek VIGRA\cite{vigra} implementiert. Die
Python-Bindings hierfür liegen nicht auf allen Systemen in gleich guter Verfügbarkeit
vor (zumindest war es uns auf Mac OS nicht möglich, diese ohne Mac Ports zum Laufen
zu bekommen, da die per \lstinline{pip install} verfügbare Version veraltet ist und
scheinbar nicht mehr gepflegt wird. (FIXME source)

Da der Temporalfilter zum einen der einzige Filter mit VIGRA Abhängigkeit war, und
der Filter zudem nicht die gewünschten Vorteile geliefert hat, die wir uns erhofft
hatten, haben wir diesen Filter nicht weiter in der Entwicklung betrachtet und so
auch die zusätzliche Dependency eingespart.

\subsection{Selbstorganisation und Arbeitsweise}

In unserer Arbeitsweise ist uns deutlich geworden, dass die Arbeit an praktischen
Projekten einen großen Lerneffekt hat und auch die zahlreichen Fehlversuche mit
wiederkehrenden frustrierenden Ergebnissen dazugehören, um schließlich zu einer
funktionierenden und robusten Implementierung zu gelangen. Um bei ähnlichen
Projekten in Zukunft schneller zu einem Ergebnis zu kommen, wird es aber nötig
sein, dass wir uns von Anfang an selbst klarere Ziele stecken und den Fortschritt
besser zu verfolgen um ihn auch sicher stellen zu können.
{\color{red}FIXME: das klingt etwas negativer. Vielleicht also nicht im Fazit nennen sondern davor noch einen Abschnitt über unsere Arbeitsweise während des Projekts?}

\section{Bewertung des Endergebnisses}

In der am Anfang gesetzten Zielsetzung waren folgende Ziele aufgeführt:

\begin{itemize}
\item Daten über die Flugbahnen von (drei) jonglierten Bällen gewinnen, wobei der Spieler in einem wohl definierten Abstand zur Kinect steht.
\item Es ist ausreichend, wenn Bälle verfolgt werden können, die von der Infrarot-Technologie erfasst werden können.
\item Die Daten sollen ausreichend sein, um auf Grundlage dessen weitere Anwendungen schreiben zu können.
\end{itemize}

Diese drei wichtigsten Ziele sind erfüllt worden. Der Spieler muss in einem bestimmten Abstand zur Kinect stehen und aus dieser Position heraus können bei einem Jongliermuster mit mindestens drei Bällen diese Bälle erkannt und zumeist gut verfolgt werden. Bälle, die zu stark reflektieren, können von der Infrarot-Technologie nicht erfasst werden. Da wir aber gerade mit den Tiefendaten, gewonnen aus der Infrarot-Strahlung, arbeiten, haben wir als Voraussetzung gegeben, dass Bälle genutzt werden sollen, die von der Kinect erfasst werden können. Die Daten, die nun intern zur Verfügung stehen, beinhalten Informationen zur Anzahl der gerade jonglierten Bälle und ihrer Wurfbahn. Zusätzlich haben wir Informationen zu den Positionen der Hände. Diese Daten sollten genügend, um auf Grundlage dessen weitere Anwendungen, die Jongliermuster analysieren oder spielerisch darstellen, schreiben zu können.

Diese Software ist dennoch noch nicht perfektioniert, und es sind Einschränkungen, sei es durch die Hardware, oder auch unsere Software, gegeben, aber auch Features, auf deren Implementation wir stolz sind. Nachfolgend werden verschiedene Aspekte aufgeführt und zum Teil bewertet:

\begin{itemize}
\item {\em Position des Spielers:}\\
Der Spieler muss in einem bestimmten Abstand zur Kinect stehen. Steht er zu weit entfernt, wird er gar nicht erfasst, steht er zu nah dran, wird zu viel erfasst. Jedoch kann er auf dieser Linie mittig im Bild sein, oder auch am Rand des Bildausschnitts stehen, ohne dass eine erkennbare Beeinflussung der Ballerkennung entsteht.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/bewertung_nahe.jpg}
    \caption{Keine vernünftige Ballerkennung, wenn der Spieler zu nahe an der Kinect steht, da die Software die Bälle an einer bestimmten Tiefe erwartet.}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/bewertung_rand.jpg}
    \caption{Funktionierende Ballerkennung mit dem Spieler an verschiedenen Positionen im Bildausschnitt.}
\end{figure}
\item {\em Jongliermuster}\\
In den Testversuchen haben wir immer ein Grundmuster jongliert, die Innenkaskade. So testeten wir am Ende einige andere Muster und stellten fest, dass solange die Bälle und Hände sichtbar sind und mehr als zwei Bälle verwendet werden, diese ebenfalls verfolgt werden können.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/bewertung_muster.jpg}
    \caption{Links: Die Außenkaskade mit drei Bällen (zwei Bälle werden hier gerade gehalten). Rechts: Drei Bälle und ein Wurf unter dem Bein durch.}
\end{figure}
\item {\em Objekte im Bild (unbewegte und bewegte)}\\
Befinden sich Objekte im Bild, die in der Tiefenebene zwischen Spieler und Kinect stehen, so kann die Robustheit unter Umständen leiden. So entstehen teilweise Schwierigkeiten, wenn Objekte vor dem Spieler stehen, wie im linken Bild von Abbildung \ref{objects} zu sehen ist. Im rechten Bild wurde ein Arm vor dem Spieler hin- und hergeschwenkt. Hierbei hat die Balldetektion erstaunlich gut geklappt und die Bälle wurden immer gefunden, sobald sie sichtbar wurden.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/bewertung_objekt.jpg}
    \caption{Links: Ein Stuhl steht vor dem Spieler im Bild. Rechts: Eine Hand wird durch das Bild geschwenkt.}
    \label{objects}
\end{figure}
\item {\em Variierende Ballanzahl}\\
In Tests funktionierte die Ballerkennung mit drei, vier und fünf Bällen. Bei der Jonglage mit zwei Bällen in einer Hand entsteht das Problem, dass die Software immer nach zwei Händen sucht. Bei der Einhandjonglage aber kann sich die zweite Hand dicht am Körper befinden, so dass sie nicht von der Software erkannt wird. Nun wird die nächste gefundene Position als Hand interpretiert, welche aber sehr wahrscheinlich ein Ball ist, so dass keine korrekte Ballerkennung stattfindet.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/bewertung_vier.jpg}
    \caption{Jonglage mit vier Bällen (es befindet sich jeweils ein Ball in der rechten Hand).}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/bewertung_fuenf.jpg}
    \caption{Jonglage mit fünf Bällen (es befindet sich ein Ball in der linken Hand).}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/bewertung_zwei.jpg}
    \caption{Jonglage mit zwei Bällen in einer Hand. Die Software muss zwei Hände zuweisen, da sich aber die linke Hand des Spielers hinter dem zu erkennenden Tiefenbereich befindet, wird fälschlicherweise einer der beiden jonglierten Bälle als Hand behandelt.}
    \label{twohands}
\end{figure}
\item {\em Handerkennung}\\
Wie auch in Abbildung \ref{twohands} zu sehen ist, werden in bestimmten Situationen die Hände nicht, bzw. an falscher Stelle erkannt. Dies kann passieren, wenn sich nicht beide Hände in dem erwarteten Bereich befinden, oder wenn andere Objekte unterhalb der Hände auf der selben Tiefe, oder näher an der Kinect, vorhanden sind. Auch während der Jonglage werden, auch unter Beachten und Vermeidung der eben genannten Faktoren, in einzelnen Frames die Hände mal auf eine Falte in der Hose o.ä. gemappt.
\item {\em Unsichtbare Bälle}\\
Als Bedingung für unsere Software wurde genannt, dass Bälle, die von der Kinect wahrgenommen werden, zu benutzen sind. Falls die Bälle zu glänzend bzw. reflektierend sind, kann es passieren, dass die Kinect an diesen Stellen keine Tiefeninformationen empfängt und solche Bälle nicht erkennt. So hatten wir in einem Test glänzende Bälle verschiedener Farbe, grün, blau, orange und gelb. Hier war festzustellen, dass die grünen und blauen Bälle erkannt, aber die orangefarbenen und gelben Bälle nicht erkannt wurden.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/bewertung_unsichtbar.jpg}
    \caption{Glänzende Bälle in orange und gelb werden von der Infrarot-Technologie der Kinect nicht wahrgenommen.}
\end{figure}
\end{itemize}

Insgesamt macht die Software einen guten Eindruck, wenn der Spieler in einem wohl definierten Abstand zur Kinect steht und möglichst keine Objekte sich vor ihm befinden. Zudem sollten die Lichtverhältnisse zu der Infrarot-Technologie der Kinect stimmig sowie das Material der Bälle von der Kinect erkennbar sein. Ist auch nur eine dieser Faktoren nicht angemessen, kann dies zu Verlusten in der Robustheit der Ballerkennung, der Handerkennung oder anderen Aspekten der Software führen.

Anfängliche Sorgen, der Code würde den Effizienzanforderungen nicht gerecht werden, sind nicht eingetreten. Die Software läuft in Echtzeit. Falls aber auf dieser Software aufbauend weitere, evtl. komplexere Funktionen implementiert werden, kann dieses Thema wieder wichtig werden. Hierfür kann beispielsweise Cython\cite{cython} oder CFFI for Python\cite{cffi} verwendet werden, um bestimmte bildverarbeitende Algorithmen, die sehr rechenintensiv sind, zu beschleunigen.

{\color{red}FIXME ANYONE Anwendungsrelevanz.}

\section{Anwendungsmöglichkeiten}

Die Grundidee des Projektes bestand in der Erstellung eines Systems zum Erkennen und
Verfolgen von Jonglierbällen in Echtzeit. Als Ziel hinter dieser Idee ist die
Möglichkeit gesehen, mit den gewonnenen Informationen verschiedene Anwendungen zu
ermöglichen. Das Projekt kann im aktuellen Zustand als Grundlage genommen werden
für einfache Programme und Spiele.

\subsection{Idee: Ein interaktiver Jongliertrainer}

Als denkbare Anwendung auf Basis der von uns erstellten Vorverarbeitung ist ein
interaktiver Jongliertrainer zu nennen. Ein solches Programm würde automatisch die
Qualität und Güte eines Jongliermusters erkennen und so über die Zeit den Fortschritt
der jonglierenden Person aufzeichnen.

Hierzu sind mehrere Grundaufgaben zu lösen, dessen Implementierung wir teilweise
bereits in unser Projekt aufgenommen haben.

\subsubsection{Objekte zählen}

Mit den erkannten momentan fliegenden Objekten haben wir als erste Anwendung einen
einfachen Zähler geschrieben, der die tatsächliche Anzahl der im Muster befindlichen Objekte zu bestimmen versucht.

Denke wäre eine Verfolgung der Objekte über einen längeren Zeitraum, um auch
unabhängig voneinander erkannte Objekte in ihrer Identität zu bestimmen. Tatsächlich
ist ein deutlich simplerer Ansatz aber bereits sehr effektiv, indem über eine
längere Anzahl Frames geschaut wird, wie viele Objekte in der Luft sind. Es wird der
Durchschnitt dieser Werte gebildet und aufgerundet. Auf diesen Wert wird dann noch
$1$ addiert, um zur tatsächlichen Anzahl zu gelangen. Dies ist der Tatsache
geschuldet, dass beim normalen Jongliermuster zu jeder Zeit mindestens ein Objekt in der Hand ruht, es kann also davon ausgegangen werden, dass maximal $n - 1$ Objekte
gleichzeitig in der Luft sind.

Ausprobieren auf echten Daten hat ergeben, dass beim Mitteln über $15$ Frames
ein zuverlässiges Bestimmen des tatsächlichen Wertes möglich ist.

\begin{lstlisting}[language=Python]
class BallCounter(object):
    """Determine the actual number of objects in the juggling pattern."""
    def __init__(self):
        self.count = None
        self.last = []
        self.length = 15 # how many frames to analyse

    def update(self, balls):
        self.last.append(len(balls))
        if len(self.last) > self.length:
            self.count = sum(self.last[::-1][:self.length]) / (self.length*1.0)
            self.count = int(math.ceil(self.count)) + 1
\end{lstlisting}


\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/ballcount-1.png}
    \vspace{-0.5cm}
    \caption{Bestimmen der tatsächlich vorhanden Ballanzahl im Jongliermuster. Momentaufnahme bei zwei fliegenden Objekten. Die erkannte Ballanzahl (drei) wird oben links angezeigt.}
    \label{ballcount-1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/ballcount-2.png}
    \vspace{-0.5cm}
    \caption{Bestimmen der tatsächlich vorhanden Ballanzahl im Jongliermuster. Momentaufnahme bei einem fliegenden Objekt. Die erkannte Ballanzahl (drei) wird oben links angezeigt.}
    \label{ballcount-2}
\end{figure}


\subsubsection{Würfe zählen}

Eine weitere Möglichkeit ist das Zählen von aufeinander folgenden Würfen. Dies ist
bereits ein Schritt in die Richtung eines Jongliertrainers, der den Fortschritt eines
Jongleurs über eine gewisse Zeit aufzeichnet und misst.

Als Herausforderung bei der Implementierung ist zuerst die Tatsache zu nennen, dass
es in den von uns bisher betrachteten Testdaten öfter vorkommt, dass ein Ball in
einem Frame nicht erkannt wird, da der Sensor hier keine zuverlässigen Informationen
liefert. Im bisherigen Modell wird dadurch das Ballobjekt verworfen und erst im
darauf folgenden Frame ein neues initialisiert. Das Zählen von Würfen stimmt
also nicht mit dem Zählen von Ball-Initialisierungen überein.

Denkbare Lösungsansätze wären hier, die zwei (oder mehr) Teile einer unterbrochenen
Wurfparabel zu erkennen und den Zwischenschritt zu interpolieren, um die gesamte
Wurfbahn als einen einzelnen Wurf aufzufassen.

Wird die zeitliche Verteilung der Würfe aufgezeichnet, lässt sich auch analysieren,
wie gleichmäßig der Jonglier-Rhythmus ist. Auch dies ist ein wichtiger Faktor

\subsubsection{Wurfhöhen messen}

Ein gutes Jongliermuster zeichnet sich durch seiner Gleichmäßigkeit aus. Dies trifft
sowohl auf

\subsection{Idee: Siteswaps erkennen}

Bisher haben wir immer über die Grundmuster beim Jonglieren gesprochen, bei denen
Bälle im gleichmäßigen Rhythmus auf gleicher Höhe immer die gleichen Wurfbahnen
durchlaufen. Tatsächlich existieren komplexere Jongliermuster, die gewissen Regeln
folgen müssen, damit in einem gleichmäßigen Rhythmus jongliert werden kann, die Bälle
verschieden hohe Flugbahnen verfolgen können, und dabei nie zwei Objekte zur gleichen
Zeit in der Hand landen.

Eine Notation zur Aufzeichnung solche Muster ist die \textit{Siteswap-Notation}, ein
entsprechend jongliertes Muster wird verkürzt als \textit{Siteswap} bezeichnet.
Vereinfacht gesagt sind Siteswaps für das Jonglieren so etwas wie Noten für Musiker.
Die Notation besteht hierbei grundlegend aus Zahlenreihen, wobei jede Stelle der
Reihe einem Wurf entspricht, der Wert an der Stelle bestimmt die Höhe des Wurfes.

Der Siteswap \textit{531} etwa besteht aus drei Würfen. Die \textit{5} entspricht
einem Wurf der Höhe von einer regulären 5-Ball-Jonglage, die \textit{3} entsprechend
der Höhe einer 3-Ball-Jonglage. Da laut offizieller Regelung (FIXME source) eine
Jonglage erst dann als Jonglieren zählt, wenn mehr Objekte als Hände beteiligt sind,
haben die Werte Zwei, Eins und Null eine Sonderbedeutung. Die Null im Siteswap
bezeichnet eine leere Position zu diesem Takt, die Eins ist ein Übergeben des
Objektes von einer in die andere Hand und die Zwei bedeutet ein Festhalten des Balles
während einer Zählzeit.

Die konkreten Wurfhöhen sind hierbei nicht von der Zahlenvorschrift festegelegt, sie
bezeichnen lediglich relative Höhenabstände zueinander.

Siteswaps sind zudem periodisch zu verstehen, können also fortlaufend jongliert
werden. Der oben betrachtete Siteswap \textit{531} ist also gleichbedeutend mit
\textit{531531531$\dots$}.

Tatsächlich tragen die Zahlen bereits viel Information, die nicht erst beim
Jonglieren sichtbar wird. So ist ein Siteswap immer nur mit einer bestimmten
Objektanzahl möglich, die dem Durchschnitt der einzelnen Wurfwerte entspricht.
Teilt man also die Quersumme von \textit{531} ($= 9$) durch die Anzahl der Würfe
($3$), sieht man, dass dies tatsächlich ein 3-Ball-Muster ist. Hierbei sind nur
ganzzahlige Ergebnisse möglich. \textit{541} etwa wäre kein valider Siteswap und ist
nicht jonglierbar.

Wie an Hand dieser kurzen Einführung bereits deutlich wird, bieten Siteswaps sich gut
zur automatischen Analyse an. Möglich und denkbar ist also eine Anwendung, die an
Hand der von unserem System gelieferten Daten automatisch Siteswaps erkennt.

Hierbei wären folgende grundlegenden Schritte notwendig.

\begin{enumerate}
\item Würfe mit ihren absoluten Wurfhöhen aufzeichnen
\item Würfe nach Abwurfzeit sortieren
\item Relative Wurfhöhen bestimmen
\item Bei möglichen Mehrdeutigkeiten das Wissen aus der Siteswap Theorie nutzen
\end{enumerate}

Mit Hilfe dieser Erkennung ist es denkbar, dem Akteur vor der Kinect Aufgaben an Hand
von Jongliermustern zu geben, die erfolgreich jongliert werden müssen. Ein Muster
kann animiert vorjongliert werden, der Akteur muss hierbei nicht einmal mit der
Zahlentheorie konfrontiert werden. So ist ein einfaches Level-basiertes System
denkbar, das einen Jonglieranfänger vom Grundmuster zu komplexeren Mustern und
sogar mehr als drei Jonglierobjekten führt.

\section{Fazit}

Im Laufe des Projektes haben wir uns intensiv mit der Echtzeitverarbeitung von
Tiefendaten auseinander gesetzt und sind über viele nicht funktionierende
Ansätze zu einem Ergebnis gekommen, das robust und mit flexibler Anzahl von
Objekten das Muster eines Jongleurs erfasst. Unsere für die erfolgreiche
Implementierung getroffenen Annahmen über den Aufbau der Kinect und die Position des
Jongleurs im Raum sind nicht all zu einschränkend, so dass einer tatsächlichen
Anwendung der entwickelten Software nichts im Wege steht.

Die Software kann als Grundlage verwendet werden für weitere Anwendungen mit
der Kinect und Jonglierbewegungen. Dies haben wir beispielhaft am Anwendungsfall
des Zählens von Jonglierbällen gezeigt und weitere mögliche Entwicklungen
vorgeschlagen und die nötigen Schritt umrissen.

Als Endergebnis des Projektes gelangen wir erfolgreich zu einer quelloffenen und flexibel
erweiterbaren Software, die robust unsere selbst gestellte Aufgabe löst. Zudem haben
bei der Arbeit mit den verwendeten Libraries viel gelernt und dem
\lstinline{libfreenect} in der Open Source Community bei der Verbesserung der
Code Basis helfen können.




\newpage

\section{Anhang}

\subsection{Verfügbarkeit des Quellcodes}

Der im Rahmen des Projekts entwickelte Quellcode ist online verfügbar unter 
\url{https://github.com/florianletsch/kinect-juggling}. Zusätzlich enthält
das Repository auch die Präsentationsmaterialien der Zwischenstandspräsentationen
sowie die LaTeX-Quellen dieses Projektberichtes.

Der Python-Quellcode sowie die begleitende Dokumentation ist unter einer Open Source License,
der MIT License \cite{mit-license}, veröffentlicht.

\subsection{Installation und Einrichten der Software}

Die entwickelte Anwendung ist in Python geschrieben, in der Entwicklung haben wir Python 2.7.6 verwendet.

Es muss die Bibliothek \textit{libfreenect} des OpenKinect Projekts \cite{openkinect} inklusive der Python Bindings eingerichtet sein. Insgesamt müssen folgende Python module verfügbar und vom installierten
Interpreter importierbar sein.

\begin{enumerate}
    \item \lstinline{freenect} (The OpenKinect library)
    \item \lstinline{numpy} (The numerical Python library)
    \item \lstinline{cv}, \lstinline{cv2} (OpenCV)
\end{enumerate}

Für die Analyse der Live-Daten muss eine Kinect angeschlossen sein. Alternativ können vorher aufgezeichnete 
Demo-Daten verwendet werden, wenn die Anwendung im Dummy-Modus gestartet wird.


\subsection{Benutzung des Programmes}

Die Software kann von der Kommandozeile mit einem Aufruf des installierten Python Interpreters auf der \lstinline{main.py} gestartet werden. 

\begin{lstlisting}[language=bash]
python main.py
\end{lstlisting}

Standardgemäß wird hier das RGB-Signal der Kinect angezeigt, sowie eine Filterkombination gewählt, die 
die Ballerkennung unserer finalen Lösung durchführt und visualisiert. Möchte man diese Filterkombination
auf zuvor aufgenommenen Demodaten ausführen, kann dies mit dem Parameter \lstinline{--dummymode} geschehen.

\begin{lstlisting}[language=bash]
python main.py --dummymode
\end{lstlisting}

Sobald mehr als dieser eine Parameter angehängt wird, wird die standardgemäße Filterkombination
nicht initialisiert, sondern stattdessen jeder Filter so initialisiert, wie es manuell durch die 
Kommandozeilenparameter festgelegt wird. Eine komplette Auflistung der verfügbaren Parameter folgt im
nächsten Abschnitt \label{sec:parameter}.

Nach Start des Programms stehen während der Laufzeit einige Tastaturkürzel zur Verfügung.

\begin{tabular}{p{5cm}p{10cm}}
\hline
{\lstinline!P!} & Livestream am aktuellen Frame pausieren\\ 
{\lstinline!SPACE!} & Screenshot des aktuellen Frames aufnehmen, wird gespeichert im {\lstinline!/snapshots!} Verzeichnis\\
Beliebige andere Taste & Programm beenden\\ \hline
\end{tabular}

Wie erwähnt, kann die Anwendung auch auf Testdaten arbeiten, wenn zum Beispiel gerade keine Kinect zur 
Verfügung steht, oder man während des Programmierens auf Videodaten eines Jongleurs zugreifen möchte.

\label{sec:record}
Um diese Daten aufzuzeichnen, ist zuerst ein leerer Ordner \lstinline{/frames} anzulegen. Die Software ist 
dann im Aufnahmemodus zu starten.

\begin{lstlisting}[language=bash]
python main.py --record
\end{lstlisting}

So lange die Anwendung läuft, werden nun sowohl die RGB- als auch die Tiefendaten in \lstinline{/frames} 
gespeichert. Jeder Frame nimmt dabei etwa 1,5 MB Speicherplatz ein, pro Sekunde fallen also 45 MB an Daten
an. Lange Aufnahmen sollten also nur bewusst durchgeführt werden, da hier schnell eine große Datenmenge 
entsteht.


\subsection{Kommandozeilenparameter}
\label{sec:parameter}

\begin{tabular}{llp{8cm}}
Parameter & Filter & Erklärung \\ \hline
{\lstinline!--dummymode!} & - & Verwendung von Testdaten statt einer angeschlossenen Kinect \\
{\lstinline!--record!} & - & Starten der Kinect im Aufnahmemodus, Verwendung siehe \ref{sec:record}\\
{\lstinline!--cutoff!} & {\lstinline!CutOffFilter!} & Tiefenbild an Hand eines festen Schwellwertes binarisieren \\
{\lstinline!--detectball!} & {\lstinline!RectsFilter!} & Vorverarbeitung für die Ballerkennung: Regionen im Tiefenbild segmentieren \\
{\lstinline!--handtracking!} & {\lstinline!HandTrackingFilter!} & Erkennen der Hände vor der Ballerkennung \\
{\lstinline!--simplehand!} & {\lstinline!SimpleHandBallFilter!} & Ballerkennung mit simpler Zuordnung von Bällen zu Ballpositionen\\
{\lstinline!--withholes!} & Ohne {\lstinline!DepthHolesFilter!} & Kein Füllen von Löchern im Tiefenbild \\
{\lstinline!--minimal!} & {\lstinline!MinimalBallFilter!} &  Zufällige Zuordnung von Bällen zueinander\\
{\lstinline!--trajectory!} & {\lstinline!TrajectoryBallFilter!} &  Simulieren von Flugbahnen der Jonglierbälle\\
{\lstinline!--temporal!} & {\lstinline!TemporalFilter!} & Temporales Filtern zur Objekterkennung\\
{\lstinline!--slowmotion!} & {\lstinline!SlowmotionFilter!} &  Slowmotion zum besseren Analysieren der gelieferten Daten\\
{\lstinline!--showdepth!} & {\lstinline!RgbDepthFilter!} & Anzeige des vorverarbeiteten Tiefenbilder
\end{tabular}



\newpage
\phantomsection
\addcontentsline{toc}{section}{Quellen}
\renewcommand{\refname}{Quellen}
\bibliographystyle{plain}
\begin{thebibliography}{99}

{\color{red}
\bibitem{FIXME}
FIXME Bibliothek evtl. etwas sortieren oder so}

%\cite{hacking}
\bibitem{hacking}
Jeff Kramer, Nicolas Burrus, Florian Echtler, Daniel Herrera C., Matt Parker\\
{\em Hacking the Kinect}\\
ISBN 978-1-4302-3867-6\\
Springer Science+Business Media New York, 2012.

\bibitem{kober}
Jens Kober$^{1,2}$, Matthew Glisson$^{2}$, and Michael Mistry$^{2,3}$,\\
{\em Playing Catch and Juggling with a Humanoid Robot}.\\
$^{2}$ Disney Research Pittsburgh, USA\\
$^{1}$ Bielefeld University, Germany,\\
$^{3}$ University of Birmingham, UK.

\bibitem{disneyresearch}
Jens Kober$^{1,2}$, Matthew Glisson$^{2}$, and Michael Mistry$^{2,3}$,\\
Disney Research Project Web Page\\
{\em Playing Catch and Juggling with a Humanoid Robot}.\\
$^{2}$ Disney Research Pittsburgh, USA\\
$^{1}$ Bielefeld University, Germany,\\
$^{3}$ University of Birmingham, UK\\
\url{http://www.disneyresearch.com/project/juggling_robot} (accessed on 2014-06-22).

\bibitem{openkinect}
{\em OpenKinect project}\\
\url{http://openkinect.org/wiki/Main_Page} (accessed on 2014-06-12).

\bibitem{libfreenect}
{\em libfreenect GitHub repository}\\
\url{https://github.com/OpenKinect/libfreenect} (accessed on 2014-06-12).

\bibitem{3dpuppetry}
Robert T. Held$^{1}$, Ankit Gupta$^{2}$, Brian Curless$^{2}$, Maneesh Agrawala$^{1}$\\
{\em 3D Puppetry: A Kinect-based Interface for 3D Animation}\\
$^{1}$ University of California, Berkeley,\\
$^{2}$ University of Washington,\\
Cambridge, Massachusetts, USA, 2012.

\bibitem{kinectavatar}
Yan Cui$^{1}$, Will Chang, Tobias Nöll$^{1}$, Didier Stricker$^{1}$\\
{\em KinectAvatar: Fully Automatic Body Capture Using a Single Kinect}\\
$^{1}$ Augmented Vision, DFKI,\\
German Research Center for Artificial Intelligence.

\bibitem{motiondeblurring}
Hyeoungho Bae, Charless C. Fowlkes, Pai H. Chou\\
{\em Accurate Motion Deblurring using Camera Motion Tracking and Scene Depth}\\
University of California, Irvine.

\bibitem{rgbzvideos}
Christian Richardt$^{1,2}$, Carsten Stoll$^{1}$, Neil A. Dodgson$^{2}$, Hans-Peter Seidel$^{1}$, Christian Theobalt$^{1}$\\
{\em Coherent Spatiotemporal Filtering, Upsampling and Rendering of RGBZ Videos}\\
$^{1}$ MPI Informatik,\\
$^{1}$ University of Cambridge,\\
EUROGRAPHICS Volume 31, Number 2, 2012.

\bibitem{vigra}
Ullrich Köthe und viele weitere Mitwirkende\\
{\em The VIGRA Computer Vision Library Version 1.10.0}\\
\url{http://ukoethe.github.io/vigra/} (accessed on 2014-06-24).

\bibitem{hough}
Richard O. Duda, Peter E. Hart,\\
{\em Use of the Hough Transformation to detect lines and curves in pictures}\\
Technical Note 36, Artificial Intelligence Center, April 1971,\\
Published in the  Comm. ACM, Vol 15, No. 1 (11-15), January 1972.\\
\url{http://www.ai.sri.com/pubs/files/tn036-duda71.pdf} (accessed on 2014-06-25).

\bibitem{opencv}
OpenCV (Open Source Computer Vision)\\
\url{http://opencv.org/} (accessed on 2014-06-25).

\bibitem{kalman}
Rudolf E. Kálmán\\
{\em A New Approach to Linear Filtering and Prediction Problems}\\
Transaction of the ASME, Journal of Basic Engineering, 35-45,\\
Research Institute for Advanced Study, Baltimore, Md., 1960.\\
\url{http://www.cs.unc.edu/~welch/kalman/media/pdf/Kalman1960.pdf} (accessed on 2014-06-26).

\bibitem{waas}
Federal Aviation Administration\\
Frequently Asked Questions - WAAS\\
\url{http://www.faa.gov/about/office_org/headquarters_offices/ato/service_units/techops/navservices/gnss/faq/waas/} (accessed on 2014-06-26).

\bibitem{egnos}
European Space Agency\\
What is EGNOS?
\url{http://www.esa.int/Our_Activities/Navigation/The_present_-_EGNOS/What_is_EGNOS} (accessed on 2014-06-26).

\bibitem{numpy}
Numpy\\
\url{http://www.numpy.org/} (accessed on 2014-06-28).

\bibitem{scipy}
Scipy\\
\url{http://www.scipy.org/} (accessed on 2014-06-28).

\bibitem{kinect2:verge}
The Verge: XBox One Release\\
\url{http://www.theverge.com/2013/6/10/4415186/xbox-one-pricing-release-date} (Stand 30.06.2014).

\bibitem{vigra}
The VIGRA Computer Vision Library\\
\url{http://ukoethe.github.io/vigra/} (Stand 29.06.2014).

\bibitem{cython}
Cython: Python C-Extensions for Python.\\
\url{http://cython.org/} (Stand 30.06.2014).

\bibitem{cffi}
CFFI for Python\\
\url{https://cffi.readthedocs.org/en/release-0.8/} (Stand 30.06.2014).

\bibitem{libraryjuggling}
Library of Juggling: Cascade\\
\url{http://www.libraryofjuggling.com/Tricks/3balltricks/Cascade.html} (Stand 30.06.2014).

\bibitem{microsoft}
Microsoft\\
\url{http://www.microsoft.com} (Stand 30.06.2014).

\bibitem{mit-license}
Open Source Initiative Open Source Initiative,\\
\emph{The MIT License (MIT)}.\\
\url{http://opensource.org/licenses/MIT} (Stand 30.06.2014).
\end{thebibliography}

\end{document}
